{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 02_train\n",
        "- Author: \n",
        "- Date: 2025-10-23\n",
        "- Goal: 모델링/학습/통계/시각화\n",
        "- Input: \n",
        "- Output: \n",
        "- Metrics: acc@val, loss@train\n",
        "- Repro: seed=42, device=auto, config=../configs/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "38c1847b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 02_train_classical_ml.ipynb\n",
        "#\n",
        "# Classical ML on Colored MNIST  (NO Neural Networks)\n",
        "#\n",
        "# Models (ALL REQUIRED):\n",
        "#   - KNN\n",
        "#   - Decision Tree\n",
        "#   - Random Forest\n",
        "#   - XGBoost\n",
        "#\n",
        "# Objectives:\n",
        "#   1) Use preprocessed Colored MNIST from 01_preprocessing_colored_mnist.ipynb\n",
        "#   2) Clear split:\n",
        "#        - Train: 모델 학습\n",
        "#        - Val  : 학습 상태/튜닝/학습곡선용 내부 검증\n",
        "#        - Test : 최종 일반화 성능 평가\n",
        "#   3) Save trained models (.joblib) so we don't retrain every run\n",
        "#   4) Generate:\n",
        "#        - Metrics (train/val/test)\n",
        "#        - Confusion matrices (val/test)\n",
        "#        - Learning curves (train vs val)\n",
        "#        - Feature importance maps (RF, XGB)\n",
        "#\n",
        "# Notes:\n",
        "#   - No Neural Networks, No SVM, No Logistic Regression\n",
        "#   - KNN uses scaled features\n",
        "#   - Tree / RF / XGB use raw features\n",
        "#   - All plots use English labels only\n",
        "# ============================================================"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "d2a597e6",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[OK] XGBoost is already installed.\n",
            "[OK] Environment ready.\n"
          ]
        }
      ],
      "source": [
        "# ------------------------------------------------------------\n",
        "# [Cell 1] Imports & environment setup\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    confusion_matrix,\n",
        ")\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import joblib  # for saving/loading trained models\n",
        "\n",
        "# XGBoost: 필수. 없으면 자동 설치.\n",
        "try:\n",
        "    from xgboost import XGBClassifier\n",
        "    print(\"[OK] XGBoost is already installed.\")\n",
        "except ImportError:\n",
        "    print(\"[INFO] xgboost not found. Installing...\")\n",
        "    import subprocess, sys\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"xgboost\"])\n",
        "    from xgboost import XGBClassifier\n",
        "    print(\"[OK] XGBoost installed.\")\n",
        "\n",
        "# Plot settings (English labels only)\n",
        "plt.rcParams[\"font.family\"] = \"DejaVu Sans\"\n",
        "plt.rcParams[\"axes.unicode_minus\"] = False\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "\n",
        "print(\"[OK] Environment ready.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "5fc50c70",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] BASE_DIR        : /Users/jaehun_jung/colored-mnist-classification\n",
            "[INFO] NPZ_PATH        : /Users/jaehun_jung/colored-mnist-classification/data/processed/colored_mnist/colored_mnist.npz\n",
            "[INFO] RESULTS_DIR     : /Users/jaehun_jung/colored-mnist-classification/results\n",
            "[INFO] METRICS_DIR     : /Users/jaehun_jung/colored-mnist-classification/results/metrics\n",
            "[INFO] FIGURES_DIR     : /Users/jaehun_jung/colored-mnist-classification/results/figures\n",
            "[INFO] MODELS_DIR      : /Users/jaehun_jung/colored-mnist-classification/results/models\n"
          ]
        }
      ],
      "source": [
        "# ------------------------------------------------------------\n",
        "# [Cell 2] Path configuration\n",
        "#   - notebooks/ 또는 repo root 어디에서 실행해도 동작\n",
        "#   - results/ 디렉토리들 준비 (Git 추적 X 권장)\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "cwd = os.getcwd()\n",
        "\n",
        "if os.path.basename(cwd) == \"notebooks\":\n",
        "    BASE_DIR = os.path.dirname(cwd)\n",
        "else:\n",
        "    BASE_DIR = cwd\n",
        "\n",
        "NPZ_PATH = os.path.join(BASE_DIR, \"data\", \"processed\", \"colored_mnist\", \"colored_mnist.npz\")\n",
        "\n",
        "RESULTS_DIR = os.path.join(BASE_DIR, \"results\")\n",
        "RESULTS_METRICS_DIR = os.path.join(RESULTS_DIR, \"metrics\")\n",
        "RESULTS_FIGURES_DIR = os.path.join(RESULTS_DIR, \"figures\")\n",
        "RESULTS_MODELS_DIR = os.path.join(RESULTS_DIR, \"models\")\n",
        "\n",
        "os.makedirs(RESULTS_METRICS_DIR, exist_ok=True)\n",
        "os.makedirs(RESULTS_FIGURES_DIR, exist_ok=True)\n",
        "os.makedirs(RESULTS_MODELS_DIR, exist_ok=True)\n",
        "\n",
        "print(\"[INFO] BASE_DIR        :\", BASE_DIR)\n",
        "print(\"[INFO] NPZ_PATH        :\", NPZ_PATH)\n",
        "print(\"[INFO] RESULTS_DIR     :\", RESULTS_DIR)\n",
        "print(\"[INFO] METRICS_DIR     :\", RESULTS_METRICS_DIR)\n",
        "print(\"[INFO] FIGURES_DIR     :\", RESULTS_FIGURES_DIR)\n",
        "print(\"[INFO] MODELS_DIR      :\", RESULTS_MODELS_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "6ffadef1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[OK] Loaded colored_mnist.npz\n",
            "  X_train     : (67093, 2352)\n",
            "  X_test      : (16774, 2352)\n",
            "  X_train_raw : (67093, 2352)\n",
            "  X_test_raw  : (16774, 2352)\n",
            "  y_digit     : (67093,) (16774,)\n",
            "  y_fg        : (67093,) (16774,)\n",
            "  y_bg        : (67093,) (16774,)\n"
          ]
        }
      ],
      "source": [
        "# ------------------------------------------------------------\n",
        "# [Cell 3] Load preprocessed dataset\n",
        "#   - 01_preprocessing_colored_mnist.ipynb 결과 사용\n",
        "#   - 이미 1차 train/test split 완료된 상태\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "if not os.path.exists(NPZ_PATH):\n",
        "    raise FileNotFoundError(\n",
        "        f\"[ERROR] {NPZ_PATH} not found.\\n\"\n",
        "        \"Run 01_preprocessing_colored_mnist.ipynb first.\"\n",
        "    )\n",
        "\n",
        "data = np.load(NPZ_PATH)\n",
        "\n",
        "# Scaled features: KNN 등 거리 기반용\n",
        "X_train = data[\"X_train\"]\n",
        "X_test = data[\"X_test\"]\n",
        "\n",
        "# Raw features: Tree / RF / XGB용\n",
        "X_train_raw = data[\"X_train_raw\"]\n",
        "X_test_raw = data[\"X_test_raw\"]\n",
        "\n",
        "# Labels\n",
        "y_digit_train = data[\"y_digit_train\"]\n",
        "y_digit_test = data[\"y_digit_test\"]\n",
        "y_fg_train = data[\"y_fg_train\"]\n",
        "y_fg_test = data[\"y_fg_test\"]\n",
        "y_bg_train = data[\"y_bg_train\"]\n",
        "y_bg_test = data[\"y_bg_test\"]\n",
        "\n",
        "# Consistency checks\n",
        "assert X_train.shape[0] == y_digit_train.shape[0] == y_fg_train.shape[0] == y_bg_train.shape[0]\n",
        "assert X_test.shape[0] == y_digit_test.shape[0] == y_fg_test.shape[0] == y_bg_test.shape[0]\n",
        "\n",
        "print(\"[OK] Loaded colored_mnist.npz\")\n",
        "print(\"  X_train     :\", X_train.shape)\n",
        "print(\"  X_test      :\", X_test.shape)\n",
        "print(\"  X_train_raw :\", X_train_raw.shape)\n",
        "print(\"  X_test_raw  :\", X_test_raw.shape)\n",
        "print(\"  y_digit     :\", y_digit_train.shape, y_digit_test.shape)\n",
        "print(\"  y_fg        :\", y_fg_train.shape, y_fg_test.shape)\n",
        "print(\"  y_bg        :\", y_bg_train.shape, y_bg_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "498dbf63",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[OK] Tasks: ['digit', 'fg_color', 'bg_color']\n"
          ]
        }
      ],
      "source": [
        "# ------------------------------------------------------------\n",
        "# [Cell 4] Task configuration\n",
        "#   - 3개 Task를 공통 구조로 처리하기 위한 dict 정의\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "COLOR_NAMES = [\"RED\", \"ORANGE\", \"YELLOW\", \"GREEN\", \"BLUE\", \"INDIGO\", \"VIOLET\"]\n",
        "\n",
        "tasks = {\n",
        "    \"digit\": {\n",
        "        \"y_train\": y_digit_train,\n",
        "        \"y_test\": y_digit_test,\n",
        "        \"class_names\": [str(i) for i in range(10)],\n",
        "    },\n",
        "    \"fg_color\": {\n",
        "        \"y_train\": y_fg_train,\n",
        "        \"y_test\": y_fg_test,\n",
        "        \"class_names\": COLOR_NAMES,\n",
        "    },\n",
        "    \"bg_color\": {\n",
        "        \"y_train\": y_bg_train,\n",
        "        \"y_test\": y_bg_test,\n",
        "        \"class_names\": COLOR_NAMES,\n",
        "    },\n",
        "}\n",
        "\n",
        "print(\"[OK] Tasks:\", list(tasks.keys()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "c3683088",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[OK] Models defined: ['KNN', 'DecisionTree', 'RandomForest', 'XGBoost']\n"
          ]
        }
      ],
      "source": [
        "# ------------------------------------------------------------\n",
        "# [Cell 5] Model factory\n",
        "#   - 공통 하이퍼파라미터 설정\n",
        "#   - use_raw_features: 어떤 X를 쓸지 명시\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "def get_models(random_state: int = RANDOM_SEED):\n",
        "    \"\"\"\n",
        "    반환:\n",
        "      models[model_name] = (model_instance, use_raw_features_flag)\n",
        "    \"\"\"\n",
        "    models = {}\n",
        "\n",
        "    # KNN (scaled)\n",
        "    models[\"KNN\"] = (\n",
        "        KNeighborsClassifier(\n",
        "            n_neighbors=7,\n",
        "            weights=\"distance\",\n",
        "            n_jobs=-1,\n",
        "        ),\n",
        "        False,\n",
        "    )\n",
        "\n",
        "    # Decision Tree (raw)\n",
        "    models[\"DecisionTree\"] = (\n",
        "        DecisionTreeClassifier(\n",
        "            max_depth=25,\n",
        "            min_samples_split=5,\n",
        "            min_samples_leaf=2,\n",
        "            random_state=random_state,\n",
        "        ),\n",
        "        True,\n",
        "    )\n",
        "\n",
        "    # Random Forest (raw)\n",
        "    models[\"RandomForest\"] = (\n",
        "        RandomForestClassifier(\n",
        "            n_estimators=300,\n",
        "            max_depth=30,\n",
        "            min_samples_split=4,\n",
        "            min_samples_leaf=2,\n",
        "            n_jobs=-1,\n",
        "            random_state=random_state,\n",
        "        ),\n",
        "        True,\n",
        "    )\n",
        "\n",
        "    # XGBoost (raw)\n",
        "    models[\"XGBoost\"] = (\n",
        "        XGBClassifier(\n",
        "            objective=\"multi:softprob\",\n",
        "            # num_class는 Task별로 동적으로 지정\n",
        "            n_estimators=400,\n",
        "            max_depth=8,\n",
        "            learning_rate=0.1,\n",
        "            subsample=0.9,\n",
        "            colsample_bytree=0.9,\n",
        "            tree_method=\"hist\",\n",
        "            eval_metric=\"mlogloss\",\n",
        "            n_jobs=-1,\n",
        "            random_state=random_state,\n",
        "        ),\n",
        "        True,\n",
        "    )\n",
        "\n",
        "    return models\n",
        "\n",
        "print(\"[OK] Models defined:\", list(get_models().keys()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "abee2d67",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CONFIG] USE_SAVED_MODELS: True\n",
            "[CONFIG] FORCE_RETRAIN   : False\n"
          ]
        }
      ],
      "source": [
        "# ------------------------------------------------------------\n",
        "# [Cell 6] Config: model saving & control\n",
        "#   - USE_SAVED_MODELS:\n",
        "#       True  → 저장된 모델이 있으면 로드해서 사용 (test 평가만 다시)\n",
        "#       False → 항상 새로 학습\n",
        "#   - FORCE_RETRAIN:\n",
        "#       True  → 저장된 모델 무시하고 항상 재학습 후 덮어쓰기\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "USE_SAVED_MODELS = True\n",
        "FORCE_RETRAIN = False\n",
        "\n",
        "print(\"[CONFIG] USE_SAVED_MODELS:\", USE_SAVED_MODELS)\n",
        "print(\"[CONFIG] FORCE_RETRAIN   :\", FORCE_RETRAIN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "29ebb702",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ------------------------------------------------------------\n",
        "# [Cell 7] Utility: metrics, confusion matrix, learning curve\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "def compute_metrics(y_true, y_pred):\n",
        "    \"\"\"Compute Accuracy, Precision, Recall, F1 (macro).\"\"\"\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    prec = precision_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
        "    rec = recall_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
        "    f1 = f1_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
        "    return acc, prec, rec, f1\n",
        "\n",
        "\n",
        "def plot_confusion_matrix(cm, class_names, title, save_path):\n",
        "    \"\"\"Draw and save confusion matrix heatmap.\"\"\"\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    sns.heatmap(\n",
        "        cm,\n",
        "        annot=True,\n",
        "        fmt=\"d\",\n",
        "        cmap=\"Blues\",\n",
        "        xticklabels=class_names,\n",
        "        yticklabels=class_names,\n",
        "        cbar=False,\n",
        "    )\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"True\")\n",
        "    plt.title(title)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=200)\n",
        "    plt.close()\n",
        "    print(f\"[OK] Saved CM → {save_path}\")\n",
        "\n",
        "\n",
        "def plot_learning_curve_sizes(\n",
        "    model_name,\n",
        "    base_model,\n",
        "    use_raw,\n",
        "    X_train_scaled,\n",
        "    X_train_raw,\n",
        "    y_train,\n",
        "    X_val_scaled,\n",
        "    X_val_raw,\n",
        "    y_val,\n",
        "    task_name,\n",
        "    save_dir,\n",
        "    random_state=RANDOM_SEED,\n",
        "):\n",
        "    \"\"\"\n",
        "    간단한 학습곡선(learning curve) 생성:\n",
        "      - train_size 비율 [0.2, 0.4, 0.6, 0.8, 1.0]\n",
        "      - 각 비율마다\n",
        "          1) 해당 크기의 train subset으로 학습\n",
        "          2) train subset 성능, val 성능 계산\n",
        "      - 결과를 하나의 plot으로 저장\n",
        "    \"\"\"\n",
        "    train_sizes = [0.2, 0.4, 0.6, 0.8, 1.0]\n",
        "    train_scores = []\n",
        "    val_scores = []\n",
        "\n",
        "    # 사용할 feature 선택\n",
        "    X_full = X_train_raw if use_raw else X_train_scaled\n",
        "    X_val = X_val_raw if use_raw else X_val_scaled\n",
        "\n",
        "    n_samples = X_full.shape[0]\n",
        "\n",
        "    for frac in train_sizes:\n",
        "        size = max(100, int(n_samples * frac))  # 최소 100개는 사용\n",
        "        idx = np.random.choice(n_samples, size=size, replace=False)\n",
        "\n",
        "        X_sub = X_full[idx]\n",
        "        y_sub = y_train[idx]\n",
        "\n",
        "        # 모델 복사 생성\n",
        "        if model_name == \"KNN\":\n",
        "            model = KNeighborsClassifier(\n",
        "                n_neighbors=base_model.n_neighbors,\n",
        "                weights=base_model.weights,\n",
        "                n_jobs=-1,\n",
        "            )\n",
        "        elif model_name == \"DecisionTree\":\n",
        "            model = DecisionTreeClassifier(\n",
        "                max_depth=base_model.max_depth,\n",
        "                min_samples_split=base_model.min_samples_split,\n",
        "                min_samples_leaf=base_model.min_samples_leaf,\n",
        "                random_state=random_state,\n",
        "            )\n",
        "        elif model_name == \"RandomForest\":\n",
        "            model = RandomForestClassifier(\n",
        "                n_estimators=base_model.n_estimators,\n",
        "                max_depth=base_model.max_depth,\n",
        "                min_samples_split=base_model.min_samples_split,\n",
        "                min_samples_leaf=base_model.min_samples_leaf,\n",
        "                n_jobs=-1,\n",
        "                random_state=random_state,\n",
        "            )\n",
        "        elif model_name == \"XGBoost\":\n",
        "            model = XGBClassifier(\n",
        "                objective=\"multi:softprob\",\n",
        "                num_class=len(np.unique(y_train)),\n",
        "                n_estimators=base_model.n_estimators,\n",
        "                max_depth=base_model.max_depth,\n",
        "                learning_rate=base_model.learning_rate,\n",
        "                subsample=base_model.subsample,\n",
        "                colsample_bytree=base_model.colsample_bytree,\n",
        "                tree_method=base_model.tree_method,\n",
        "                eval_metric=base_model.eval_metric,\n",
        "                n_jobs=-1,\n",
        "                random_state=random_state,\n",
        "            )\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "        # 학습\n",
        "        model.fit(X_sub, y_sub)\n",
        "\n",
        "        # train subset 성능\n",
        "        if model_name == \"XGBoost\":\n",
        "            y_sub_pred = np.argmax(model.predict_proba(X_sub), axis=1)\n",
        "        else:\n",
        "            y_sub_pred = model.predict(X_sub)\n",
        "        _, _, _, f1_tr = compute_metrics(y_sub, y_sub_pred)\n",
        "\n",
        "        # val 성능\n",
        "        if model_name == \"XGBoost\":\n",
        "            y_val_pred = np.argmax(model.predict_proba(X_val), axis=1)\n",
        "        else:\n",
        "            y_val_pred = model.predict(X_val)\n",
        "        _, _, _, f1_val = compute_metrics(y_val, y_val_pred)\n",
        "\n",
        "        train_scores.append(f1_tr)\n",
        "        val_scores.append(f1_val)\n",
        "\n",
        "    # Plot\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    plt.plot(train_sizes, train_scores, marker=\"o\", label=\"Train F1 (subset)\")\n",
        "    plt.plot(train_sizes, val_scores, marker=\"o\", label=\"Val F1\")\n",
        "    plt.xlabel(\"Train size fraction\")\n",
        "    plt.ylabel(\"Macro F1\")\n",
        "    plt.ylim(0.0, 1.05)\n",
        "    plt.title(f\"Learning Curve - {task_name} - {model_name}\")\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    save_path = os.path.join(save_dir, f\"lc_{task_name}_{model_name}.png\")\n",
        "    plt.savefig(save_path, dpi=200)\n",
        "    plt.close()\n",
        "    print(f\"[OK] Saved learning curve → {save_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "a28dde79",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================== Task: digit ====================\n",
            "[INFO] Train_sub: 53674 / Val: 13419\n",
            "\n",
            "[MODEL] KNN (use_raw_features=False)\n",
            "[TRAIN] Fitting on train_sub for train/val evaluation...\n",
            "[VAL]  Acc=0.8116  Prec=0.8372  Rec=0.8084  F1=0.8140\n",
            "[OK] Saved CM → /Users/jaehun_jung/colored-mnist-classification/results/figures/cm_val_digit_KNN.png\n",
            "[OK] Saved learning curve → /Users/jaehun_jung/colored-mnist-classification/results/figures/lc_digit_KNN.png\n",
            "[LOAD] Loaded final model from /Users/jaehun_jung/colored-mnist-classification/results/models/digit_KNN.joblib\n",
            "[TEST] Acc=0.8219  Prec=0.8442  Rec=0.8191  F1=0.8241\n",
            "[OK] Saved CM → /Users/jaehun_jung/colored-mnist-classification/results/figures/cm_test_digit_KNN.png\n",
            "\n",
            "[MODEL] DecisionTree (use_raw_features=True)\n",
            "[TRAIN] Fitting on train_sub for train/val evaluation...\n",
            "[VAL]  Acc=0.4820  Prec=0.6170  Rec=0.4766  F1=0.5060\n",
            "[OK] Saved CM → /Users/jaehun_jung/colored-mnist-classification/results/figures/cm_val_digit_DecisionTree.png\n",
            "[OK] Saved learning curve → /Users/jaehun_jung/colored-mnist-classification/results/figures/lc_digit_DecisionTree.png\n",
            "[LOAD] Loaded final model from /Users/jaehun_jung/colored-mnist-classification/results/models/digit_DecisionTree.joblib\n",
            "[TEST] Acc=0.5357  Prec=0.5921  Rec=0.5319  F1=0.5414\n",
            "[OK] Saved CM → /Users/jaehun_jung/colored-mnist-classification/results/figures/cm_test_digit_DecisionTree.png\n",
            "\n",
            "[MODEL] RandomForest (use_raw_features=True)\n",
            "[TRAIN] Fitting on train_sub for train/val evaluation...\n",
            "[VAL]  Acc=0.8985  Prec=0.8990  Rec=0.8971  F1=0.8974\n",
            "[OK] Saved CM → /Users/jaehun_jung/colored-mnist-classification/results/figures/cm_val_digit_RandomForest.png\n",
            "[OK] Saved learning curve → /Users/jaehun_jung/colored-mnist-classification/results/figures/lc_digit_RandomForest.png\n",
            "[LOAD] Loaded final model from /Users/jaehun_jung/colored-mnist-classification/results/models/digit_RandomForest.joblib\n",
            "[TEST] Acc=0.9051  Prec=0.9052  Rec=0.9039  F1=0.9041\n",
            "[OK] Saved CM → /Users/jaehun_jung/colored-mnist-classification/results/figures/cm_test_digit_RandomForest.png\n",
            "\n",
            "[MODEL] XGBoost (use_raw_features=True)\n",
            "[TRAIN] Fitting on train_sub for train/val evaluation...\n",
            "[VAL]  Acc=0.9321  Prec=0.9320  Rec=0.9315  F1=0.9316\n",
            "[OK] Saved CM → /Users/jaehun_jung/colored-mnist-classification/results/figures/cm_val_digit_XGBoost.png\n",
            "[OK] Saved learning curve → /Users/jaehun_jung/colored-mnist-classification/results/figures/lc_digit_XGBoost.png\n",
            "[TRAIN] Fitting FINAL model on FULL train set...\n",
            "[SAVE] Saved final model → /Users/jaehun_jung/colored-mnist-classification/results/models/digit_XGBoost.joblib\n",
            "[TEST] Acc=0.9368  Prec=0.9367  Rec=0.9363  F1=0.9364\n",
            "[OK] Saved CM → /Users/jaehun_jung/colored-mnist-classification/results/figures/cm_test_digit_XGBoost.png\n",
            "\n",
            "==================== Task: fg_color ====================\n",
            "[INFO] Train_sub: 53674 / Val: 13419\n",
            "\n",
            "[MODEL] KNN (use_raw_features=False)\n",
            "[TRAIN] Fitting on train_sub for train/val evaluation...\n",
            "[VAL]  Acc=0.7904  Prec=0.8490  Rec=0.7917  F1=0.7941\n",
            "[OK] Saved CM → /Users/jaehun_jung/colored-mnist-classification/results/figures/cm_val_fg_color_KNN.png\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[32], line 156\u001b[0m\n\u001b[1;32m    148\u001b[0m plot_confusion_matrix(\n\u001b[1;32m    149\u001b[0m     cm_val,\n\u001b[1;32m    150\u001b[0m     class_names,\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCM (Val) - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtask_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    152\u001b[0m     cm_val_path,\n\u001b[1;32m    153\u001b[0m )\n\u001b[1;32m    155\u001b[0m \u001b[38;5;66;03m# Learning Curve 저장\u001b[39;00m\n\u001b[0;32m--> 156\u001b[0m plot_learning_curve_sizes(\n\u001b[1;32m    157\u001b[0m     model_name,\n\u001b[1;32m    158\u001b[0m     base_model,\n\u001b[1;32m    159\u001b[0m     use_raw,\n\u001b[1;32m    160\u001b[0m     X_tr_scaled,\n\u001b[1;32m    161\u001b[0m     X_tr_raw,\n\u001b[1;32m    162\u001b[0m     y_tr,\n\u001b[1;32m    163\u001b[0m     X_val_scaled,\n\u001b[1;32m    164\u001b[0m     X_val_raw,\n\u001b[1;32m    165\u001b[0m     y_val,\n\u001b[1;32m    166\u001b[0m     task_name,\n\u001b[1;32m    167\u001b[0m     RESULTS_FIGURES_DIR,\n\u001b[1;32m    168\u001b[0m     random_state\u001b[38;5;241m=\u001b[39mRANDOM_SEED,\n\u001b[1;32m    169\u001b[0m )\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# ---------- 2) Final model: FULL train으로 학습 or 로드 후 TEST 평가 ----------\u001b[39;00m\n\u001b[1;32m    172\u001b[0m final_model_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[1;32m    173\u001b[0m     RESULTS_MODELS_DIR,\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtask_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.joblib\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    175\u001b[0m )\n",
            "Cell \u001b[0;32mIn[31], line 121\u001b[0m, in \u001b[0;36mplot_learning_curve_sizes\u001b[0;34m(model_name, base_model, use_raw, X_train_scaled, X_train_raw, y_train, X_val_scaled, X_val_raw, y_val, task_name, save_dir, random_state)\u001b[0m\n\u001b[1;32m    119\u001b[0m     y_sub_pred \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(model\u001b[38;5;241m.\u001b[39mpredict_proba(X_sub), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 121\u001b[0m     y_sub_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_sub)\n\u001b[1;32m    122\u001b[0m _, _, _, f1_tr \u001b[38;5;241m=\u001b[39m compute_metrics(y_sub, y_sub_pred)\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# val 성능\u001b[39;00m\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/neighbors/_classification.py:274\u001b[0m, in \u001b[0;36mKNeighborsClassifier.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    272\u001b[0m     neigh_dist \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 274\u001b[0m     neigh_dist, neigh_ind \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkneighbors(X)\n\u001b[1;32m    276\u001b[0m classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_\n\u001b[1;32m    277\u001b[0m _y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_y\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/neighbors/_base.py:849\u001b[0m, in \u001b[0;36mKNeighborsMixin.kneighbors\u001b[0;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[1;32m    842\u001b[0m use_pairwise_distances_reductions \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    843\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbrute\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    844\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m ArgKmin\u001b[38;5;241m.\u001b[39mis_usable_for(\n\u001b[1;32m    845\u001b[0m         X \u001b[38;5;28;01mif\u001b[39;00m X \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meffective_metric_\n\u001b[1;32m    846\u001b[0m     )\n\u001b[1;32m    847\u001b[0m )\n\u001b[1;32m    848\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_pairwise_distances_reductions:\n\u001b[0;32m--> 849\u001b[0m     results \u001b[38;5;241m=\u001b[39m ArgKmin\u001b[38;5;241m.\u001b[39mcompute(\n\u001b[1;32m    850\u001b[0m         X\u001b[38;5;241m=\u001b[39mX,\n\u001b[1;32m    851\u001b[0m         Y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_X,\n\u001b[1;32m    852\u001b[0m         k\u001b[38;5;241m=\u001b[39mn_neighbors,\n\u001b[1;32m    853\u001b[0m         metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meffective_metric_,\n\u001b[1;32m    854\u001b[0m         metric_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meffective_metric_params_,\n\u001b[1;32m    855\u001b[0m         strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    856\u001b[0m         return_distance\u001b[38;5;241m=\u001b[39mreturn_distance,\n\u001b[1;32m    857\u001b[0m     )\n\u001b[1;32m    859\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[1;32m    860\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbrute\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetric \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprecomputed\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m issparse(X)\n\u001b[1;32m    861\u001b[0m ):\n\u001b[1;32m    862\u001b[0m     results \u001b[38;5;241m=\u001b[39m _kneighbors_from_graph(\n\u001b[1;32m    863\u001b[0m         X, n_neighbors\u001b[38;5;241m=\u001b[39mn_neighbors, return_distance\u001b[38;5;241m=\u001b[39mreturn_distance\n\u001b[1;32m    864\u001b[0m     )\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_pairwise_distances_reduction/_dispatcher.py:290\u001b[0m, in \u001b[0;36mArgKmin.compute\u001b[0;34m(cls, X, Y, k, metric, chunk_size, metric_kwargs, strategy, return_distance)\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ArgKmin64\u001b[38;5;241m.\u001b[39mcompute(\n\u001b[1;32m    279\u001b[0m         X\u001b[38;5;241m=\u001b[39mX,\n\u001b[1;32m    280\u001b[0m         Y\u001b[38;5;241m=\u001b[39mY,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    286\u001b[0m         return_distance\u001b[38;5;241m=\u001b[39mreturn_distance,\n\u001b[1;32m    287\u001b[0m     )\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m Y\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mfloat32:\n\u001b[0;32m--> 290\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ArgKmin32\u001b[38;5;241m.\u001b[39mcompute(\n\u001b[1;32m    291\u001b[0m         X\u001b[38;5;241m=\u001b[39mX,\n\u001b[1;32m    292\u001b[0m         Y\u001b[38;5;241m=\u001b[39mY,\n\u001b[1;32m    293\u001b[0m         k\u001b[38;5;241m=\u001b[39mk,\n\u001b[1;32m    294\u001b[0m         metric\u001b[38;5;241m=\u001b[39mmetric,\n\u001b[1;32m    295\u001b[0m         chunk_size\u001b[38;5;241m=\u001b[39mchunk_size,\n\u001b[1;32m    296\u001b[0m         metric_kwargs\u001b[38;5;241m=\u001b[39mmetric_kwargs,\n\u001b[1;32m    297\u001b[0m         strategy\u001b[38;5;241m=\u001b[39mstrategy,\n\u001b[1;32m    298\u001b[0m         return_distance\u001b[38;5;241m=\u001b[39mreturn_distance,\n\u001b[1;32m    299\u001b[0m     )\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnly float64 or float32 datasets pairs are supported at this time, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgot: X.dtype=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and Y.dtype=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mY\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    304\u001b[0m )\n",
            "File \u001b[0;32msklearn/metrics/_pairwise_distances_reduction/_argkmin.pyx:550\u001b[0m, in \u001b[0;36msklearn.metrics._pairwise_distances_reduction._argkmin.ArgKmin32.compute\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/threadpoolctl.py:592\u001b[0m, in \u001b[0;36m_ThreadpoolLimiter.__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m    589\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__enter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    590\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[0;32m--> 592\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mtype\u001b[39m, value, traceback):\n\u001b[1;32m    593\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrestore_original_limits()\n\u001b[1;32m    595\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    596\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrap\u001b[39m(\u001b[38;5;28mcls\u001b[39m, controller, \u001b[38;5;241m*\u001b[39m, limits\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, user_api\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# ------------------------------------------------------------\n",
        "# [Cell 8] Train/Val/Test workflow with model saving\n",
        "#\n",
        "# For each task & model:\n",
        "#   1) Split TRAIN into (train_sub, val)\n",
        "#   2) Fit model on train_sub → eval on train_sub & val\n",
        "#      - store metrics (set=train, val)\n",
        "#      - save CM for val\n",
        "#      - generate learning curve (train vs val)\n",
        "#   3) Train or load FINAL model on FULL train → eval on test\n",
        "#      - save model (.joblib)\n",
        "#      - store metrics (set=test)\n",
        "#      - save CM for test\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "models_template = get_models()\n",
        "all_results = []\n",
        "trained_models = {task: {} for task in tasks.keys()}\n",
        "\n",
        "VAL_RATIO = 0.2  # train -> (train_sub 80%, val 20%)\n",
        "\n",
        "for task_name, tinfo in tasks.items():\n",
        "    print(f\"\\n==================== Task: {task_name} ====================\")\n",
        "\n",
        "    y_train_all = tinfo[\"y_train\"]\n",
        "    y_test = tinfo[\"y_test\"]\n",
        "    class_names = tinfo[\"class_names\"]\n",
        "    num_classes = len(class_names)\n",
        "\n",
        "    n_train = X_train.shape[0]\n",
        "    indices = np.arange(n_train)\n",
        "\n",
        "    # --- Task별로 stratified train/val split 인덱스 생성 ---\n",
        "    train_idx, val_idx = train_test_split(\n",
        "        indices,\n",
        "        test_size=VAL_RATIO,\n",
        "        random_state=RANDOM_SEED,\n",
        "        stratify=y_train_all,\n",
        "    )\n",
        "\n",
        "    # scaled features\n",
        "    X_tr_scaled = X_train[train_idx]\n",
        "    X_val_scaled = X_train[val_idx]\n",
        "\n",
        "    # raw features\n",
        "    X_tr_raw = X_train_raw[train_idx]\n",
        "    X_val_raw = X_train_raw[val_idx]\n",
        "\n",
        "    y_tr = y_train_all[train_idx]\n",
        "    y_val = y_train_all[val_idx]\n",
        "\n",
        "    print(f\"[INFO] Train_sub: {X_tr_scaled.shape[0]} / Val: {X_val_scaled.shape[0]}\")\n",
        "\n",
        "    for model_name, (base_model, use_raw) in models_template.items():\n",
        "        print(f\"\\n[MODEL] {model_name} (use_raw_features={use_raw})\")\n",
        "\n",
        "        # ---------- 1) Train_sub / Val 학습 및 평가 ----------\n",
        "        # 학습에 사용할 feature 선택\n",
        "        Xtr_sub = X_tr_raw if use_raw else X_tr_scaled\n",
        "        Xval = X_val_raw if use_raw else X_val_scaled\n",
        "\n",
        "        # base_model을 복사하여 task-specific model 생성\n",
        "        if model_name == \"KNN\":\n",
        "            model_sub = KNeighborsClassifier(\n",
        "                n_neighbors=base_model.n_neighbors,\n",
        "                weights=base_model.weights,\n",
        "                n_jobs=-1,\n",
        "            )\n",
        "        elif model_name == \"DecisionTree\":\n",
        "            model_sub = DecisionTreeClassifier(\n",
        "                max_depth=base_model.max_depth,\n",
        "                min_samples_split=base_model.min_samples_split,\n",
        "                min_samples_leaf=base_model.min_samples_leaf,\n",
        "                random_state=RANDOM_SEED,\n",
        "            )\n",
        "        elif model_name == \"RandomForest\":\n",
        "            model_sub = RandomForestClassifier(\n",
        "                n_estimators=base_model.n_estimators,\n",
        "                max_depth=base_model.max_depth,\n",
        "                min_samples_split=base_model.min_samples_split,\n",
        "                min_samples_leaf=base_model.min_samples_leaf,\n",
        "                n_jobs=-1,\n",
        "                random_state=RANDOM_SEED,\n",
        "            )\n",
        "        elif model_name == \"XGBoost\":\n",
        "            model_sub = XGBClassifier(\n",
        "                objective=\"multi:softprob\",\n",
        "                num_class=num_classes,\n",
        "                n_estimators=base_model.n_estimators,\n",
        "                max_depth=base_model.max_depth,\n",
        "                learning_rate=base_model.learning_rate,\n",
        "                subsample=base_model.subsample,\n",
        "                colsample_bytree=base_model.colsample_bytree,\n",
        "                tree_method=base_model.tree_method,\n",
        "                eval_metric=base_model.eval_metric,\n",
        "                n_jobs=-1,\n",
        "                random_state=RANDOM_SEED,\n",
        "            )\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown model: {model_name}\")\n",
        "\n",
        "        # Train on train_sub\n",
        "        print(\"[TRAIN] Fitting on train_sub for train/val evaluation...\")\n",
        "        model_sub.fit(Xtr_sub, y_tr)\n",
        "\n",
        "        # Train_sub performance\n",
        "        if model_name == \"XGBoost\":\n",
        "            y_tr_pred = np.argmax(model_sub.predict_proba(Xtr_sub), axis=1)\n",
        "        else:\n",
        "            y_tr_pred = model_sub.predict(Xtr_sub)\n",
        "        acc_tr, prec_tr, rec_tr, f1_tr = compute_metrics(y_tr, y_tr_pred)\n",
        "\n",
        "        all_results.append({\n",
        "            \"task\": task_name,\n",
        "            \"model\": model_name,\n",
        "            \"set\": \"train_sub\",\n",
        "            \"accuracy\": acc_tr,\n",
        "            \"precision_macro\": prec_tr,\n",
        "            \"recall_macro\": rec_tr,\n",
        "            \"f1_macro\": f1_tr,\n",
        "        })\n",
        "\n",
        "        # Val performance\n",
        "        if model_name == \"XGBoost\":\n",
        "            y_val_pred = np.argmax(model_sub.predict_proba(Xval), axis=1)\n",
        "        else:\n",
        "            y_val_pred = model_sub.predict(Xval)\n",
        "        acc_v, prec_v, rec_v, f1_v = compute_metrics(y_val, y_val_pred)\n",
        "\n",
        "        print(f\"[VAL]  Acc={acc_v:.4f}  Prec={prec_v:.4f}  Rec={rec_v:.4f}  F1={f1_v:.4f}\")\n",
        "\n",
        "        all_results.append({\n",
        "            \"task\": task_name,\n",
        "            \"model\": model_name,\n",
        "            \"set\": \"val\",\n",
        "            \"accuracy\": acc_v,\n",
        "            \"precision_macro\": prec_v,\n",
        "            \"recall_macro\": rec_v,\n",
        "            \"f1_macro\": f1_v,\n",
        "        })\n",
        "\n",
        "        # Val Confusion Matrix 저장\n",
        "        cm_val = confusion_matrix(y_val, y_val_pred)\n",
        "        cm_val_path = os.path.join(\n",
        "            RESULTS_FIGURES_DIR,\n",
        "            f\"cm_val_{task_name}_{model_name}.png\",\n",
        "        )\n",
        "        plot_confusion_matrix(\n",
        "            cm_val,\n",
        "            class_names,\n",
        "            f\"CM (Val) - {task_name} - {model_name}\",\n",
        "            cm_val_path,\n",
        "        )\n",
        "\n",
        "        # Learning Curve 저장\n",
        "        plot_learning_curve_sizes(\n",
        "            model_name,\n",
        "            base_model,\n",
        "            use_raw,\n",
        "            X_tr_scaled,\n",
        "            X_tr_raw,\n",
        "            y_tr,\n",
        "            X_val_scaled,\n",
        "            X_val_raw,\n",
        "            y_val,\n",
        "            task_name,\n",
        "            RESULTS_FIGURES_DIR,\n",
        "            random_state=RANDOM_SEED,\n",
        "        )\n",
        "\n",
        "        # ---------- 2) Final model: FULL train으로 학습 or 로드 후 TEST 평가 ----------\n",
        "        final_model_path = os.path.join(\n",
        "            RESULTS_MODELS_DIR,\n",
        "            f\"{task_name}_{model_name}.joblib\",\n",
        "        )\n",
        "\n",
        "        # feature for full train / test\n",
        "        Xtrain_full = X_train_raw if use_raw else X_train\n",
        "        Xtest_full = X_test_raw if use_raw else X_test\n",
        "\n",
        "        if USE_SAVED_MODELS and os.path.exists(final_model_path) and not FORCE_RETRAIN:\n",
        "            # 기존 학습된 최종 모델 로드\n",
        "            final_model = joblib.load(final_model_path)\n",
        "            print(f\"[LOAD] Loaded final model from {final_model_path}\")\n",
        "        else:\n",
        "            # 새 최종 모델 학습 (train_sub + val → 즉 전체 train 사용)\n",
        "            if model_name == \"KNN\":\n",
        "                final_model = KNeighborsClassifier(\n",
        "                    n_neighbors=base_model.n_neighbors,\n",
        "                    weights=base_model.weights,\n",
        "                    n_jobs=-1,\n",
        "                )\n",
        "            elif model_name == \"DecisionTree\":\n",
        "                final_model = DecisionTreeClassifier(\n",
        "                    max_depth=base_model.max_depth,\n",
        "                    min_samples_split=base_model.min_samples_split,\n",
        "                    min_samples_leaf=base_model.min_samples_leaf,\n",
        "                    random_state=RANDOM_SEED,\n",
        "                )\n",
        "            elif model_name == \"RandomForest\":\n",
        "                final_model = RandomForestClassifier(\n",
        "                    n_estimators=base_model.n_estimators,\n",
        "                    max_depth=base_model.max_depth,\n",
        "                    min_samples_split=base_model.min_samples_split,\n",
        "                    min_samples_leaf=base_model.min_samples_leaf,\n",
        "                    n_jobs=-1,\n",
        "                    random_state=RANDOM_SEED,\n",
        "                )\n",
        "            elif model_name == \"XGBoost\":\n",
        "                final_model = XGBClassifier(\n",
        "                    objective=\"multi:softprob\",\n",
        "                    num_class=num_classes,\n",
        "                    n_estimators=base_model.n_estimators,\n",
        "                    max_depth=base_model.max_depth,\n",
        "                    learning_rate=base_model.learning_rate,\n",
        "                    subsample=base_model.subsample,\n",
        "                    colsample_bytree=base_model.colsample_bytree,\n",
        "                    tree_method=base_model.tree_method,\n",
        "                    eval_metric=base_model.eval_metric,\n",
        "                    n_jobs=-1,\n",
        "                    random_state=RANDOM_SEED,\n",
        "                )\n",
        "            else:\n",
        "                raise ValueError(f\"Unknown model: {model_name}\")\n",
        "\n",
        "            print(\"[TRAIN] Fitting FINAL model on FULL train set...\")\n",
        "            final_model.fit(Xtrain_full, y_train_all)\n",
        "            joblib.dump(final_model, final_model_path)\n",
        "            print(f\"[SAVE] Saved final model → {final_model_path}\")\n",
        "\n",
        "        # Test set 평가\n",
        "        if model_name == \"XGBoost\":\n",
        "            y_test_pred = np.argmax(final_model.predict_proba(Xtest_full), axis=1)\n",
        "        else:\n",
        "            y_test_pred = final_model.predict(Xtest_full)\n",
        "\n",
        "        acc_te, prec_te, rec_te, f1_te = compute_metrics(y_test, y_test_pred)\n",
        "\n",
        "        print(f\"[TEST] Acc={acc_te:.4f}  Prec={prec_te:.4f}  Rec={rec_te:.4f}  F1={f1_te:.4f}\")\n",
        "\n",
        "        all_results.append({\n",
        "            \"task\": task_name,\n",
        "            \"model\": model_name,\n",
        "            \"set\": \"test\",\n",
        "            \"accuracy\": acc_te,\n",
        "            \"precision_macro\": prec_te,\n",
        "            \"recall_macro\": rec_te,\n",
        "            \"f1_macro\": f1_te,\n",
        "        })\n",
        "\n",
        "        # Test Confusion Matrix\n",
        "        cm_test = confusion_matrix(y_test, y_test_pred)\n",
        "        cm_test_path = os.path.join(\n",
        "            RESULTS_FIGURES_DIR,\n",
        "            f\"cm_test_{task_name}_{model_name}.png\",\n",
        "        )\n",
        "        plot_confusion_matrix(\n",
        "            cm_test,\n",
        "            class_names,\n",
        "            f\"CM (Test) - {task_name} - {model_name}\",\n",
        "            cm_test_path,\n",
        "        )\n",
        "\n",
        "        # 최종 모델 저장 (importance에서 사용)\n",
        "        trained_models[task_name][model_name] = final_model\n",
        "\n",
        "print(\"\\n[OK] Train_sub/Val/Test evaluation completed for all models.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67e3aac0",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ------------------------------------------------------------\n",
        "# [Cell 9] Save metrics summary\n",
        "#   - train_sub / val / test 모두 포함\n",
        "#   - 03_analysis_report.ipynb에서 이 파일을 읽어 분석\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "results_df = pd.DataFrame(all_results)\n",
        "\n",
        "# 정렬: task → set → f1_macro (내림차순)\n",
        "results_df = results_df.sort_values(\n",
        "    by=[\"task\", \"set\", \"f1_macro\"],\n",
        "    ascending=[True, True, False],\n",
        ")\n",
        "\n",
        "summary_path = os.path.join(RESULTS_METRICS_DIR, \"classical_ml_summary.csv\")\n",
        "results_df.to_csv(summary_path, index=False)\n",
        "\n",
        "print(\"[OK] Saved full metrics summary →\", summary_path)\n",
        "display(results_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee35d2fe",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ------------------------------------------------------------\n",
        "# [Cell 10] Feature importance (RF & XGB, using final models)\n",
        "#   - Test에 사용된 최종 모델 기반으로 중요도 시각화\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "for task_name, tinfo in tasks.items():\n",
        "    class_names = tinfo[\"class_names\"]\n",
        "\n",
        "    print(f\"\\n==================== Feature Importance: {task_name} ====================\")\n",
        "\n",
        "    # RandomForest\n",
        "    rf_model = trained_models[task_name].get(\"RandomForest\", None)\n",
        "    if rf_model is not None and hasattr(rf_model, \"feature_importances_\"):\n",
        "        rf_imp = rf_model.feature_importances_\n",
        "        rf_path = os.path.join(\n",
        "            RESULTS_FIGURES_DIR,\n",
        "            f\"fi_{task_name}_rf.png\",\n",
        "        )\n",
        "        plot_importance_map(\n",
        "            rf_imp,\n",
        "            f\"RF importance - {task_name}\",\n",
        "            rf_path,\n",
        "        )\n",
        "    else:\n",
        "        print(\"[INFO] RandomForest not available or no feature_importances_.\")\n",
        "\n",
        "    # XGBoost\n",
        "    xgb_model = trained_models[task_name].get(\"XGBoost\", None)\n",
        "    if xgb_model is not None and hasattr(xgb_model, \"feature_importances_\"):\n",
        "        xgb_imp = xgb_model.feature_importances_\n",
        "        xgb_path = os.path.join(\n",
        "            RESULTS_FIGURES_DIR,\n",
        "            f\"fi_{task_name}_xgb.png\",\n",
        "        )\n",
        "        plot_importance_map(\n",
        "            xgb_imp,\n",
        "            f\"XGB importance - {task_name}\",\n",
        "            xgb_path,\n",
        "        )\n",
        "    else:\n",
        "        print(\"[INFO] XGBoost not available or no feature_importances_.\")\n",
        "\n",
        "print(\"\\n✅ Finished 02_train_classical_ml.ipynb\")\n",
        "print(\"   - Models: saved in results/models/\")\n",
        "print(\"   - Metrics: results/metrics/classical_ml_summary.csv\")\n",
        "print(\"   - Figures: results/figures/ (CM, LC, FI)\")\n",
        "print(\"   - Next: use 03_analysis_report.ipynb for narrative analysis.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f37351af",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
