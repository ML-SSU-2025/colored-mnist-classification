{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 02_train\n",
        "- Author: \n",
        "- Date: 2025-10-23\n",
        "- Goal: 모델링/학습/통계/시각화\n",
        "- Input: \n",
        "- Output: \n",
        "- Metrics: acc@val, loss@train\n",
        "- Repro: seed=42, device=auto, config=../configs/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "38c1847b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 02_train_classical_ml.ipynb\n",
        "#\n",
        "# Classical ML on Colored MNIST  (NO Neural Networks)\n",
        "#\n",
        "# Models (ALL REQUIRED):\n",
        "#   - KNN\n",
        "#   - Decision Tree\n",
        "#   - Random Forest\n",
        "#   - XGBoost\n",
        "#\n",
        "# Objectives:\n",
        "#   1) Use preprocessed Colored MNIST from 01_preprocessing_colored_mnist.ipynb\n",
        "#   2) Clear split:\n",
        "#        - Train: 모델 학습\n",
        "#        - Val  : 학습 상태/튜닝/학습곡선용 내부 검증\n",
        "#        - Test : 최종 일반화 성능 평가\n",
        "#   3) Save trained models (.joblib) so we don't retrain every run\n",
        "#   4) Generate:\n",
        "#        - Metrics (train/val/test)\n",
        "#        - Confusion matrices (val/test)\n",
        "#        - Learning curves (train vs val)\n",
        "#        - Feature importance maps (RF, XGB)\n",
        "#\n",
        "# Notes:\n",
        "#   - No Neural Networks, No SVM, No Logistic Regression\n",
        "#   - KNN uses scaled features\n",
        "#   - Tree / RF / XGB use raw features\n",
        "#   - All plots use English labels only\n",
        "# ============================================================"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "d2a597e6",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[OK] XGBoost is already installed.\n",
            "[OK] Environment ready.\n"
          ]
        }
      ],
      "source": [
        "# ------------------------------------------------------------\n",
        "# [Cell 1] Imports & environment setup\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    confusion_matrix,\n",
        ")\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import joblib  # for saving/loading trained models\n",
        "\n",
        "# XGBoost: 필수. 없으면 자동 설치.\n",
        "try:\n",
        "    from xgboost import XGBClassifier\n",
        "    print(\"[OK] XGBoost is already installed.\")\n",
        "except ImportError:\n",
        "    print(\"[INFO] xgboost not found. Installing...\")\n",
        "    import subprocess, sys\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"xgboost\"])\n",
        "    from xgboost import XGBClassifier\n",
        "    print(\"[OK] XGBoost installed.\")\n",
        "\n",
        "# Plot settings (English labels only)\n",
        "plt.rcParams[\"font.family\"] = \"DejaVu Sans\"\n",
        "plt.rcParams[\"axes.unicode_minus\"] = False\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "\n",
        "print(\"[OK] Environment ready.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "5fc50c70",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] BASE_DIR        : c:\\src\\colored-mnist-classification\n",
            "[INFO] NPZ_PATH        : c:\\src\\colored-mnist-classification\\data\\processed\\colored_mnist\\colored_mnist.npz\n",
            "[INFO] RESULTS_DIR     : c:\\src\\colored-mnist-classification\\results\n",
            "[INFO] METRICS_DIR     : c:\\src\\colored-mnist-classification\\results\\metrics\n",
            "[INFO] FIGURES_DIR     : c:\\src\\colored-mnist-classification\\results\\figures\n",
            "[INFO] MODELS_DIR      : c:\\src\\colored-mnist-classification\\results\\models\n"
          ]
        }
      ],
      "source": [
        "# ------------------------------------------------------------\n",
        "# [Cell 2] Path configuration\n",
        "#   - notebooks/ 또는 repo root 어디에서 실행해도 동작\n",
        "#   - results/ 디렉토리들 준비 (Git 추적 X 권장)\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "cwd = os.getcwd()\n",
        "\n",
        "if os.path.basename(cwd) == \"notebooks\":\n",
        "    BASE_DIR = os.path.dirname(cwd)\n",
        "else:\n",
        "    BASE_DIR = cwd\n",
        "\n",
        "NPZ_PATH = os.path.join(BASE_DIR, \"data\", \"processed\", \"colored_mnist\", \"colored_mnist.npz\")\n",
        "\n",
        "RESULTS_DIR = os.path.join(BASE_DIR, \"results\")\n",
        "RESULTS_METRICS_DIR = os.path.join(RESULTS_DIR, \"metrics\")\n",
        "RESULTS_FIGURES_DIR = os.path.join(RESULTS_DIR, \"figures\")\n",
        "RESULTS_MODELS_DIR = os.path.join(RESULTS_DIR, \"models\")\n",
        "\n",
        "os.makedirs(RESULTS_METRICS_DIR, exist_ok=True)\n",
        "os.makedirs(RESULTS_FIGURES_DIR, exist_ok=True)\n",
        "os.makedirs(RESULTS_MODELS_DIR, exist_ok=True)\n",
        "\n",
        "print(\"[INFO] BASE_DIR        :\", BASE_DIR)\n",
        "print(\"[INFO] NPZ_PATH        :\", NPZ_PATH)\n",
        "print(\"[INFO] RESULTS_DIR     :\", RESULTS_DIR)\n",
        "print(\"[INFO] METRICS_DIR     :\", RESULTS_METRICS_DIR)\n",
        "print(\"[INFO] FIGURES_DIR     :\", RESULTS_FIGURES_DIR)\n",
        "print(\"[INFO] MODELS_DIR      :\", RESULTS_MODELS_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "6ffadef1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[OK] Loaded colored_mnist.npz\n",
            "  X_train     : (62293, 2352)\n",
            "  X_test      : (15574, 2352)\n",
            "  X_train_raw : (62293, 2352)\n",
            "  X_test_raw  : (15574, 2352)\n",
            "  y_digit     : (62293,) (15574,)\n",
            "  y_fg        : (62293,) (15574,)\n",
            "  y_bg        : (62293,) (15574,)\n"
          ]
        }
      ],
      "source": [
        "# ------------------------------------------------------------\n",
        "# [Cell 3] Load preprocessed dataset\n",
        "#   - 01_preprocessing_colored_mnist.ipynb 결과 사용\n",
        "#   - 이미 1차 train/test split 완료된 상태\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "if not os.path.exists(NPZ_PATH):\n",
        "    raise FileNotFoundError(\n",
        "        f\"[ERROR] {NPZ_PATH} not found.\\n\"\n",
        "        \"Run 01_preprocessing_colored_mnist.ipynb first.\"\n",
        "    )\n",
        "\n",
        "data = np.load(NPZ_PATH)\n",
        "\n",
        "# Scaled features: KNN 등 거리 기반용\n",
        "X_train = data[\"X_train\"]\n",
        "X_test = data[\"X_test\"]\n",
        "\n",
        "# Raw features: Tree / RF / XGB용\n",
        "X_train_raw = data[\"X_train_raw\"]\n",
        "X_test_raw = data[\"X_test_raw\"]\n",
        "\n",
        "# Labels\n",
        "y_digit_train = data[\"y_digit_train\"]\n",
        "y_digit_test = data[\"y_digit_test\"]\n",
        "y_fg_train = data[\"y_fg_train\"]\n",
        "y_fg_test = data[\"y_fg_test\"]\n",
        "y_bg_train = data[\"y_bg_train\"]\n",
        "y_bg_test = data[\"y_bg_test\"]\n",
        "\n",
        "# Consistency checks\n",
        "assert X_train.shape[0] == y_digit_train.shape[0] == y_fg_train.shape[0] == y_bg_train.shape[0]\n",
        "assert X_test.shape[0] == y_digit_test.shape[0] == y_fg_test.shape[0] == y_bg_test.shape[0]\n",
        "\n",
        "print(\"[OK] Loaded colored_mnist.npz\")\n",
        "print(\"  X_train     :\", X_train.shape)\n",
        "print(\"  X_test      :\", X_test.shape)\n",
        "print(\"  X_train_raw :\", X_train_raw.shape)\n",
        "print(\"  X_test_raw  :\", X_test_raw.shape)\n",
        "print(\"  y_digit     :\", y_digit_train.shape, y_digit_test.shape)\n",
        "print(\"  y_fg        :\", y_fg_train.shape, y_fg_test.shape)\n",
        "print(\"  y_bg        :\", y_bg_train.shape, y_bg_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "498dbf63",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[OK] Tasks: ['digit', 'fg_color', 'bg_color']\n"
          ]
        }
      ],
      "source": [
        "# ------------------------------------------------------------\n",
        "# [Cell 4] Task configuration\n",
        "#   - 3개 Task를 공통 구조로 처리하기 위한 dict 정의\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "COLOR_NAMES = [\"RED\", \"ORANGE\", \"YELLOW\", \"GREEN\", \"BLUE\", \"INDIGO\", \"VIOLET\"]\n",
        "\n",
        "tasks = {\n",
        "    \"digit\": {\n",
        "        \"y_train\": y_digit_train,\n",
        "        \"y_test\": y_digit_test,\n",
        "        \"class_names\": [str(i) for i in range(10)],\n",
        "    },\n",
        "    \"fg_color\": {\n",
        "        \"y_train\": y_fg_train,\n",
        "        \"y_test\": y_fg_test,\n",
        "        \"class_names\": COLOR_NAMES,\n",
        "    },\n",
        "    \"bg_color\": {\n",
        "        \"y_train\": y_bg_train,\n",
        "        \"y_test\": y_bg_test,\n",
        "        \"class_names\": COLOR_NAMES,\n",
        "    },\n",
        "}\n",
        "\n",
        "print(\"[OK] Tasks:\", list(tasks.keys()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "c3683088",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[OK] Models defined: ['SVM_RBF']\n"
          ]
        }
      ],
      "source": [
        "# ------------------------------------------------------------\n",
        "# [Cell 5] Model factory\n",
        "#   - 공통 하이퍼파라미터 설정\n",
        "#   - use_raw_features: 어떤 X를 쓸지 명시\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "def get_models(random_state: int = RANDOM_SEED):\n",
        "    \"\"\"\n",
        "    반환:\n",
        "      models[model_name] = (model_instance, use_raw_features_flag)\n",
        "    \"\"\"\n",
        "    models = {}\n",
        "\n",
        "    # SVM (scaled, RBF kernel)\n",
        "    models[\"SVM_RBF\"] = (\n",
        "        SVC(\n",
        "            kernel=\"rbf\",\n",
        "            C=1,\n",
        "            gamma=\"scale\",\n",
        "            probability=False,  \n",
        "            random_state=random_state,\n",
        "        ),\n",
        "        False,  \n",
        "    )\n",
        "\n",
        "    # # KNN (scaled)\n",
        "    # models[\"KNN\"] = (\n",
        "    #     KNeighborsClassifier(\n",
        "    #         n_neighbors=7,\n",
        "    #         weights=\"distance\",\n",
        "    #         n_jobs=-1,\n",
        "    #     ),\n",
        "    #     False,\n",
        "    # )\n",
        "\n",
        "    # # Decision Tree (raw)\n",
        "    # models[\"DecisionTree\"] = (\n",
        "    #     DecisionTreeClassifier(\n",
        "    #         max_depth=25,\n",
        "    #         min_samples_split=5,\n",
        "    #         min_samples_leaf=2,\n",
        "    #         random_state=random_state,\n",
        "    #     ),\n",
        "    #     True,\n",
        "    # )\n",
        "\n",
        "    # # Random Forest (raw)\n",
        "    # models[\"RandomForest\"] = (\n",
        "    #     RandomForestClassifier(\n",
        "    #         n_estimators=300,\n",
        "    #         max_depth=30,\n",
        "    #         min_samples_split=4,\n",
        "    #         min_samples_leaf=2,\n",
        "    #         n_jobs=-1,\n",
        "    #         random_state=random_state,\n",
        "    #     ),\n",
        "    #     True,\n",
        "    # )\n",
        "\n",
        "    # # XGBoost (raw)\n",
        "    # models[\"XGBoost\"] = (\n",
        "    #     XGBClassifier(\n",
        "    #         objective=\"multi:softprob\",\n",
        "    #         # num_class는 Task별로 동적으로 지정\n",
        "    #         n_estimators=400,\n",
        "    #         max_depth=8,\n",
        "    #         learning_rate=0.1,\n",
        "    #         subsample=0.9,\n",
        "    #         colsample_bytree=0.9,\n",
        "    #         tree_method=\"hist\",\n",
        "    #         eval_metric=\"mlogloss\",\n",
        "    #         n_jobs=-1,\n",
        "    #         random_state=random_state,\n",
        "    #     ),\n",
        "    #     True,\n",
        "    # )\n",
        "\n",
        "    return models\n",
        "\n",
        "print(\"[OK] Models defined:\", list(get_models().keys()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "abee2d67",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CONFIG] USE_SAVED_MODELS: True\n",
            "[CONFIG] FORCE_RETRAIN   : False\n"
          ]
        }
      ],
      "source": [
        "# ------------------------------------------------------------\n",
        "# [Cell 6] Config: model saving & control\n",
        "#   - USE_SAVED_MODELS:\n",
        "#       True  → 저장된 모델이 있으면 로드해서 사용 (test 평가만 다시)\n",
        "#       False → 항상 새로 학습\n",
        "#   - FORCE_RETRAIN:\n",
        "#       True  → 저장된 모델 무시하고 항상 재학습 후 덮어쓰기\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "USE_SAVED_MODELS = True\n",
        "FORCE_RETRAIN = False\n",
        "\n",
        "print(\"[CONFIG] USE_SAVED_MODELS:\", USE_SAVED_MODELS)\n",
        "print(\"[CONFIG] FORCE_RETRAIN   :\", FORCE_RETRAIN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "29ebb702",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ------------------------------------------------------------\n",
        "# [Cell 7] Utility: metrics, confusion matrix, learning curve\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "def compute_metrics(y_true, y_pred):\n",
        "    \"\"\"Compute Accuracy, Precision, Recall, F1 (macro).\"\"\"\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    prec = precision_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
        "    rec = recall_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
        "    f1 = f1_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
        "    return acc, prec, rec, f1\n",
        "\n",
        "\n",
        "def plot_confusion_matrix(cm, class_names, title, save_path):\n",
        "    \"\"\"Draw and save confusion matrix heatmap.\"\"\"\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    sns.heatmap(\n",
        "        cm,\n",
        "        annot=True,\n",
        "        fmt=\"d\",\n",
        "        cmap=\"Blues\",\n",
        "        xticklabels=class_names,\n",
        "        yticklabels=class_names,\n",
        "        cbar=False,\n",
        "    )\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"True\")\n",
        "    plt.title(title)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=200)\n",
        "    plt.close()\n",
        "    print(f\"[OK] Saved CM → {save_path}\")\n",
        "\n",
        "\n",
        "def plot_learning_curve_sizes(\n",
        "    model_name,\n",
        "    base_model,\n",
        "    use_raw,\n",
        "    X_train_scaled,\n",
        "    X_train_raw,\n",
        "    y_train,\n",
        "    X_val_scaled,\n",
        "    X_val_raw,\n",
        "    y_val,\n",
        "    task_name,\n",
        "    save_dir,\n",
        "    random_state=RANDOM_SEED,\n",
        "):\n",
        "    \"\"\"\n",
        "    간단한 학습곡선(learning curve) 생성:\n",
        "      - train_size 비율 [0.2, 0.4, 0.6, 0.8, 1.0]\n",
        "      - 각 비율마다\n",
        "          1) 해당 크기의 train subset으로 학습\n",
        "          2) train subset 성능, val 성능 계산\n",
        "      - 결과를 하나의 plot으로 저장\n",
        "    \"\"\"\n",
        "    train_sizes = [0.2, 0.4, 0.6, 0.8, 1.0]\n",
        "    train_scores = []\n",
        "    val_scores = []\n",
        "\n",
        "    # 사용할 feature 선택\n",
        "    X_full = X_train_raw if use_raw else X_train_scaled\n",
        "    X_val = X_val_raw if use_raw else X_val_scaled\n",
        "\n",
        "    n_samples = X_full.shape[0]\n",
        "\n",
        "    for frac in train_sizes:\n",
        "        size = max(100, int(n_samples * frac))  # 최소 100개는 사용\n",
        "        idx = np.random.choice(n_samples, size=size, replace=False)\n",
        "\n",
        "        X_sub = X_full[idx]\n",
        "        y_sub = y_train[idx]\n",
        "\n",
        "        # 모델 복사 생성\n",
        "        if model_name == \"SVM_RBF\":\n",
        "            model = SVC(\n",
        "                kernel=base_model.kernel,\n",
        "                C=base_model.C,\n",
        "                gamma=base_model.gamma,\n",
        "                probability=base_model.probability,\n",
        "                random_state=random_state,\n",
        "            )\n",
        "        # if model_name == \"KNN\":\n",
        "        #     model = KNeighborsClassifier(\n",
        "        #         n_neighbors=base_model.n_neighbors,\n",
        "        #         weights=base_model.weights,\n",
        "        #         n_jobs=-1,\n",
        "        #     )\n",
        "        # elif model_name == \"DecisionTree\":\n",
        "        #     model = DecisionTreeClassifier(\n",
        "        #         max_depth=base_model.max_depth,\n",
        "        #         min_samples_split=base_model.min_samples_split,\n",
        "        #         min_samples_leaf=base_model.min_samples_leaf,\n",
        "        #         random_state=random_state,\n",
        "        #     )\n",
        "        # elif model_name == \"RandomForest\":\n",
        "        #     model = RandomForestClassifier(\n",
        "        #         n_estimators=base_model.n_estimators,\n",
        "        #         max_depth=base_model.max_depth,\n",
        "        #         min_samples_split=base_model.min_samples_split,\n",
        "        #         min_samples_leaf=base_model.min_samples_leaf,\n",
        "        #         n_jobs=-1,\n",
        "        #         random_state=random_state,\n",
        "        #     )\n",
        "        # elif model_name == \"XGBoost\":\n",
        "        #     model = XGBClassifier(\n",
        "        #         objective=\"multi:softprob\",\n",
        "        #         num_class=len(np.unique(y_train)),\n",
        "        #         n_estimators=base_model.n_estimators,\n",
        "        #         max_depth=base_model.max_depth,\n",
        "        #         learning_rate=base_model.learning_rate,\n",
        "        #         subsample=base_model.subsample,\n",
        "        #         colsample_bytree=base_model.colsample_bytree,\n",
        "        #         tree_method=base_model.tree_method,\n",
        "        #         eval_metric=base_model.eval_metric,\n",
        "        #         n_jobs=-1,\n",
        "        #         random_state=random_state,\n",
        "        #     )\n",
        "            \n",
        "        else:\n",
        "            continue\n",
        "\n",
        "        # 학습\n",
        "        model.fit(X_sub, y_sub)\n",
        "\n",
        "        # train subset 성능\n",
        "        if model_name == \"XGBoost\":\n",
        "            y_sub_pred = np.argmax(model.predict_proba(X_sub), axis=1)\n",
        "        else:\n",
        "            y_sub_pred = model.predict(X_sub)\n",
        "        _, _, _, f1_tr = compute_metrics(y_sub, y_sub_pred)\n",
        "\n",
        "        # val 성능\n",
        "        if model_name == \"XGBoost\":\n",
        "            y_val_pred = np.argmax(model.predict_proba(X_val), axis=1)\n",
        "        else:\n",
        "            y_val_pred = model.predict(X_val)\n",
        "        _, _, _, f1_val = compute_metrics(y_val, y_val_pred)\n",
        "\n",
        "        train_scores.append(f1_tr)\n",
        "        val_scores.append(f1_val)\n",
        "\n",
        "    # Plot\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    plt.plot(train_sizes, train_scores, marker=\"o\", label=\"Train F1 (subset)\")\n",
        "    plt.plot(train_sizes, val_scores, marker=\"o\", label=\"Val F1\")\n",
        "    plt.xlabel(\"Train size fraction\")\n",
        "    plt.ylabel(\"Macro F1\")\n",
        "    plt.ylim(0.0, 1.05)\n",
        "    plt.title(f\"Learning Curve - {task_name} - {model_name}\")\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    save_path = os.path.join(save_dir, f\"lc_{task_name}_{model_name}.png\")\n",
        "    plt.savefig(save_path, dpi=200)\n",
        "    plt.close()\n",
        "    print(f\"[OK] Saved learning curve → {save_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a28dde79",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================== Task: digit ====================\n",
            "[INFO] Train_sub: 49834 / Val: 12459\n",
            "\n",
            "[MODEL] SVM_RBF (use_raw_features=False)\n",
            "[TRAIN] Fitting on train_sub for train/val evaluation...\n",
            "[VAL]  Acc=0.8107  Prec=0.8335  Rec=0.8077  F1=0.8140\n",
            "[OK] Saved CM → c:\\src\\colored-mnist-classification\\results\\figures\\cm_val_digit_SVM_RBF.png\n",
            "[TRAIN] Fitting FINAL model on FULL train set...\n",
            "[SAVE] Saved final model → c:\\src\\colored-mnist-classification\\results\\models\\digit_SVM_RBF.joblib\n"
          ]
        }
      ],
      "source": [
        "# ------------------------------------------------------------\n",
        "# [Cell 8] Train/Val/Test workflow with model saving\n",
        "#\n",
        "# For each task & model:\n",
        "#   1) Split TRAIN into (train_sub, val)\n",
        "#   2) Fit model on train_sub → eval on train_sub & val\n",
        "#      - store metrics (set=train, val)\n",
        "#      - save CM for val\n",
        "#      - generate learning curve (train vs val)\n",
        "#   3) Train or load FINAL model on FULL train → eval on test\n",
        "#      - save model (.joblib)\n",
        "#      - store metrics (set=test)\n",
        "#      - save CM for test\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "models_template = get_models()\n",
        "all_results = []\n",
        "trained_models = {task: {} for task in tasks.keys()}\n",
        "\n",
        "VAL_RATIO = 0.2  # train -> (train_sub 80%, val 20%)\n",
        "\n",
        "for task_name, tinfo in tasks.items():\n",
        "    print(f\"\\n==================== Task: {task_name} ====================\")\n",
        "\n",
        "    y_train_all = tinfo[\"y_train\"]\n",
        "    y_test = tinfo[\"y_test\"]\n",
        "    class_names = tinfo[\"class_names\"]\n",
        "    num_classes = len(class_names)\n",
        "\n",
        "    n_train = X_train.shape[0]\n",
        "    indices = np.arange(n_train)\n",
        "\n",
        "    # --- Task별로 stratified train/val split 인덱스 생성 ---\n",
        "    train_idx, val_idx = train_test_split(\n",
        "        indices,\n",
        "        test_size=VAL_RATIO,\n",
        "        random_state=RANDOM_SEED,\n",
        "        stratify=y_train_all,\n",
        "    )\n",
        "\n",
        "    # scaled features\n",
        "    X_tr_scaled = X_train[train_idx]\n",
        "    X_val_scaled = X_train[val_idx]\n",
        "\n",
        "    # raw features\n",
        "    X_tr_raw = X_train_raw[train_idx]\n",
        "    X_val_raw = X_train_raw[val_idx]\n",
        "\n",
        "    y_tr = y_train_all[train_idx]\n",
        "    y_val = y_train_all[val_idx]\n",
        "\n",
        "    print(f\"[INFO] Train_sub: {X_tr_scaled.shape[0]} / Val: {X_val_scaled.shape[0]}\")\n",
        "\n",
        "    for model_name, (base_model, use_raw) in models_template.items():\n",
        "        print(f\"\\n[MODEL] {model_name} (use_raw_features={use_raw})\")\n",
        "\n",
        "        # ---------- 1) Train_sub / Val 학습 및 평가 ----------\n",
        "        # 학습에 사용할 feature 선택\n",
        "        Xtr_sub = X_tr_raw if use_raw else X_tr_scaled\n",
        "        Xval = X_val_raw if use_raw else X_val_scaled\n",
        "\n",
        "        # base_model을 복사하여 task-specific model 생성\n",
        "        if model_name == \"SVM_RBF\":\n",
        "            model_sub = SVC(\n",
        "                kernel=base_model.kernel,\n",
        "                C=base_model.C,\n",
        "                gamma=base_model.gamma,\n",
        "                probability=base_model.probability,\n",
        "                random_state=RANDOM_SEED,\n",
        "            )\n",
        "        # if model_name == \"KNN\":\n",
        "        #     model_sub = KNeighborsClassifier(\n",
        "        #         n_neighbors=base_model.n_neighbors,\n",
        "        #         weights=base_model.weights,\n",
        "        #         n_jobs=-1,\n",
        "        #     )\n",
        "        # elif model_name == \"DecisionTree\":\n",
        "        #     model_sub = DecisionTreeClassifier(\n",
        "        #         max_depth=base_model.max_depth,\n",
        "        #         min_samples_split=base_model.min_samples_split,\n",
        "        #         min_samples_leaf=base_model.min_samples_leaf,\n",
        "        #         random_state=RANDOM_SEED,\n",
        "        #     )\n",
        "        # elif model_name == \"RandomForest\":\n",
        "        #     model_sub = RandomForestClassifier(\n",
        "        #         n_estimators=base_model.n_estimators,\n",
        "        #         max_depth=base_model.max_depth,\n",
        "        #         min_samples_split=base_model.min_samples_split,\n",
        "        #         min_samples_leaf=base_model.min_samples_leaf,\n",
        "        #         n_jobs=-1,\n",
        "        #         random_state=RANDOM_SEED,\n",
        "        #     )\n",
        "        # elif model_name == \"XGBoost\":\n",
        "        #     model_sub = XGBClassifier(\n",
        "        #         objective=\"multi:softprob\",\n",
        "        #         num_class=num_classes,\n",
        "        #         n_estimators=base_model.n_estimators,\n",
        "        #         max_depth=base_model.max_depth,\n",
        "        #         learning_rate=base_model.learning_rate,\n",
        "        #         subsample=base_model.subsample,\n",
        "        #         colsample_bytree=base_model.colsample_bytree,\n",
        "        #         tree_method=base_model.tree_method,\n",
        "        #         eval_metric=base_model.eval_metric,\n",
        "        #         n_jobs=-1,\n",
        "        #         random_state=RANDOM_SEED,\n",
        "        #     )\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown model: {model_name}\")\n",
        "\n",
        "        # Train on train_sub\n",
        "        print(\"[TRAIN] Fitting on train_sub for train/val evaluation...\")\n",
        "        model_sub.fit(Xtr_sub, y_tr)\n",
        "\n",
        "        # Train_sub performance\n",
        "        if model_name == \"XGBoost\":\n",
        "            y_tr_pred = np.argmax(model_sub.predict_proba(Xtr_sub), axis=1)\n",
        "        else:\n",
        "            y_tr_pred = model_sub.predict(Xtr_sub)\n",
        "        acc_tr, prec_tr, rec_tr, f1_tr = compute_metrics(y_tr, y_tr_pred)\n",
        "\n",
        "        all_results.append({\n",
        "            \"task\": task_name,\n",
        "            \"model\": model_name,\n",
        "            \"set\": \"train_sub\",\n",
        "            \"accuracy\": acc_tr,\n",
        "            \"precision_macro\": prec_tr,\n",
        "            \"recall_macro\": rec_tr,\n",
        "            \"f1_macro\": f1_tr,\n",
        "        })\n",
        "\n",
        "        # Val performance\n",
        "        if model_name == \"XGBoost\":\n",
        "            y_val_pred = np.argmax(model_sub.predict_proba(Xval), axis=1)\n",
        "        else:\n",
        "            y_val_pred = model_sub.predict(Xval)\n",
        "        acc_v, prec_v, rec_v, f1_v = compute_metrics(y_val, y_val_pred)\n",
        "\n",
        "        print(f\"[VAL]  Acc={acc_v:.4f}  Prec={prec_v:.4f}  Rec={rec_v:.4f}  F1={f1_v:.4f}\")\n",
        "\n",
        "        all_results.append({\n",
        "            \"task\": task_name,\n",
        "            \"model\": model_name,\n",
        "            \"set\": \"val\",\n",
        "            \"accuracy\": acc_v,\n",
        "            \"precision_macro\": prec_v,\n",
        "            \"recall_macro\": rec_v,\n",
        "            \"f1_macro\": f1_v,\n",
        "        })\n",
        "\n",
        "        # Val Confusion Matrix 저장\n",
        "        cm_val = confusion_matrix(y_val, y_val_pred)\n",
        "        cm_val_path = os.path.join(\n",
        "            RESULTS_FIGURES_DIR,\n",
        "            f\"cm_val_{task_name}_{model_name}.png\",\n",
        "        )\n",
        "        plot_confusion_matrix(\n",
        "            cm_val,\n",
        "            class_names,\n",
        "            f\"CM (Val) - {task_name} - {model_name}\",\n",
        "            cm_val_path,\n",
        "        )\n",
        "\n",
        "        # # Learning Curve 저장\n",
        "        # plot_learning_curve_sizes(\n",
        "        #     model_name,\n",
        "        #     base_model,\n",
        "        #     use_raw,\n",
        "        #     X_tr_scaled,\n",
        "        #     X_tr_raw,\n",
        "        #     y_tr,\n",
        "        #     X_val_scaled,\n",
        "        #     X_val_raw,\n",
        "        #     y_val,\n",
        "        #     task_name,\n",
        "        #     RESULTS_FIGURES_DIR,\n",
        "        #     random_state=RANDOM_SEED,\n",
        "        # )\n",
        "\n",
        "        # ---------- 2) Final model: FULL train으로 학습 or 로드 후 TEST 평가 ----------\n",
        "        final_model_path = os.path.join(\n",
        "            RESULTS_MODELS_DIR,\n",
        "            f\"{task_name}_{model_name}.joblib\",\n",
        "        )\n",
        "\n",
        "        # feature for full train / test\n",
        "        Xtrain_full = X_train_raw if use_raw else X_train\n",
        "        Xtest_full = X_test_raw if use_raw else X_test\n",
        "\n",
        "        if USE_SAVED_MODELS and os.path.exists(final_model_path) and not FORCE_RETRAIN:\n",
        "            # 기존 학습된 최종 모델 로드\n",
        "            final_model = joblib.load(final_model_path)\n",
        "            print(f\"[LOAD] Loaded final model from {final_model_path}\")\n",
        "        else:\n",
        "            # 새 최종 모델 학습 (train_sub + val → 즉 전체 train 사용)\n",
        "            if model_name == \"SVM_RBF\":\n",
        "                final_model = SVC(\n",
        "                    kernel=base_model.kernel,\n",
        "                    C=base_model.C,\n",
        "                    gamma=base_model.gamma,\n",
        "                    probability=base_model.probability,\n",
        "                    random_state=RANDOM_SEED,\n",
        "                )\n",
        "            # if model_name == \"KNN\":\n",
        "            #     final_model = KNeighborsClassifier(\n",
        "            #         n_neighbors=base_model.n_neighbors,\n",
        "            #         weights=base_model.weights,\n",
        "            #         n_jobs=-1,\n",
        "            #     )\n",
        "            # elif model_name == \"DecisionTree\":\n",
        "            #     final_model = DecisionTreeClassifier(\n",
        "            #         max_depth=base_model.max_depth,\n",
        "            #         min_samples_split=base_model.min_samples_split,\n",
        "            #         min_samples_leaf=base_model.min_samples_leaf,\n",
        "            #         random_state=RANDOM_SEED,\n",
        "            #     )\n",
        "            # elif model_name == \"RandomForest\":\n",
        "            #     final_model = RandomForestClassifier(\n",
        "            #         n_estimators=base_model.n_estimators,\n",
        "            #         max_depth=base_model.max_depth,\n",
        "            #         min_samples_split=base_model.min_samples_split,\n",
        "            #         min_samples_leaf=base_model.min_samples_leaf,\n",
        "            #         n_jobs=-1,\n",
        "            #         random_state=RANDOM_SEED,\n",
        "            #     )\n",
        "            # elif model_name == \"XGBoost\":\n",
        "            #     final_model = XGBClassifier(\n",
        "            #         objective=\"multi:softprob\",\n",
        "            #         num_class=num_classes,\n",
        "            #         n_estimators=base_model.n_estimators,\n",
        "            #         max_depth=base_model.max_depth,\n",
        "            #         learning_rate=base_model.learning_rate,\n",
        "            #         subsample=base_model.subsample,\n",
        "            #         colsample_bytree=base_model.colsample_bytree,\n",
        "            #         tree_method=base_model.tree_method,\n",
        "            #         eval_metric=base_model.eval_metric,\n",
        "            #         n_jobs=-1,\n",
        "            #         random_state=RANDOM_SEED,\n",
        "            #     )\n",
        "            else:\n",
        "                raise ValueError(f\"Unknown model: {model_name}\")\n",
        "\n",
        "            print(\"[TRAIN] Fitting FINAL model on FULL train set...\")\n",
        "            final_model.fit(Xtrain_full, y_train_all)\n",
        "            joblib.dump(final_model, final_model_path)\n",
        "            print(f\"[SAVE] Saved final model → {final_model_path}\")\n",
        "\n",
        "        # Test set 평가\n",
        "        if model_name == \"XGBoost\":\n",
        "            y_test_pred = np.argmax(final_model.predict_proba(Xtest_full), axis=1)\n",
        "        else:\n",
        "            y_test_pred = final_model.predict(Xtest_full)\n",
        "\n",
        "        acc_te, prec_te, rec_te, f1_te = compute_metrics(y_test, y_test_pred)\n",
        "\n",
        "        print(f\"[TEST] Acc={acc_te:.4f}  Prec={prec_te:.4f}  Rec={rec_te:.4f}  F1={f1_te:.4f}\")\n",
        "\n",
        "        all_results.append({\n",
        "            \"task\": task_name,\n",
        "            \"model\": model_name,\n",
        "            \"set\": \"test\",\n",
        "            \"accuracy\": acc_te,\n",
        "            \"precision_macro\": prec_te,\n",
        "            \"recall_macro\": rec_te,\n",
        "            \"f1_macro\": f1_te,\n",
        "        })\n",
        "\n",
        "        # Test Confusion Matrix\n",
        "        cm_test = confusion_matrix(y_test, y_test_pred)\n",
        "        cm_test_path = os.path.join(\n",
        "            RESULTS_FIGURES_DIR,\n",
        "            f\"cm_test_{task_name}_{model_name}.png\",\n",
        "        )\n",
        "        plot_confusion_matrix(\n",
        "            cm_test,\n",
        "            class_names,\n",
        "            f\"CM (Test) - {task_name} - {model_name}\",\n",
        "            cm_test_path,\n",
        "        )\n",
        "\n",
        "        # 최종 모델 저장 (importance에서 사용)\n",
        "        trained_models[task_name][model_name] = final_model\n",
        "\n",
        "print(\"\\n[OK] Train_sub/Val/Test evaluation completed for all models.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67e3aac0",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ------------------------------------------------------------\n",
        "# [Cell 9] Save metrics summary\n",
        "#   - train_sub / val / test 모두 포함\n",
        "#   - 03_analysis_report.ipynb에서 이 파일을 읽어 분석\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "results_df = pd.DataFrame(all_results)\n",
        "\n",
        "# 정렬: task → set → f1_macro (내림차순)\n",
        "results_df = results_df.sort_values(\n",
        "    by=[\"task\", \"set\", \"f1_macro\"],\n",
        "    ascending=[True, True, False],\n",
        ")\n",
        "\n",
        "summary_path = os.path.join(RESULTS_METRICS_DIR, \"classical_ml_summary.csv\")\n",
        "results_df.to_csv(summary_path, index=False)\n",
        "\n",
        "print(\"[OK] Saved full metrics summary →\", summary_path)\n",
        "display(results_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee35d2fe",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ------------------------------------------------------------\n",
        "# [Cell 10] Feature importance (RF & XGB, using final models)\n",
        "#   - Test에 사용된 최종 모델 기반으로 중요도 시각화\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "for task_name, tinfo in tasks.items():\n",
        "    # class_names = tinfo[\"class_names\"]\n",
        "\n",
        "    print(f\"\\n==================== Feature Importance: {task_name} ====================\")\n",
        "    print(\"[INFO] SVM does not have feature_importances_. Skipping...\")\n",
        "\n",
        "    # # RandomForest\n",
        "    # rf_model = trained_models[task_name].get(\"RandomForest\", None)\n",
        "    # if rf_model is not None and hasattr(rf_model, \"feature_importances_\"):\n",
        "    #     rf_imp = rf_model.feature_importances_\n",
        "    #     rf_path = os.path.join(\n",
        "    #         RESULTS_FIGURES_DIR,\n",
        "    #         f\"fi_{task_name}_rf.png\",\n",
        "    #     )\n",
        "    #     plot_importance_map(\n",
        "    #         rf_imp,\n",
        "    #         f\"RF importance - {task_name}\",\n",
        "    #         rf_path,\n",
        "    #     )\n",
        "    # else:\n",
        "    #     print(\"[INFO] RandomForest not available or no feature_importances_.\")\n",
        "\n",
        "    # # XGBoost\n",
        "    # xgb_model = trained_models[task_name].get(\"XGBoost\", None)\n",
        "    # if xgb_model is not None and hasattr(xgb_model, \"feature_importances_\"):\n",
        "    #     xgb_imp = xgb_model.feature_importances_\n",
        "    #     xgb_path = os.path.join(\n",
        "    #         RESULTS_FIGURES_DIR,\n",
        "    #         f\"fi_{task_name}_xgb.png\",\n",
        "    #     )\n",
        "    #     plot_importance_map(\n",
        "    #         xgb_imp,\n",
        "    #         f\"XGB importance - {task_name}\",\n",
        "    #         xgb_path,\n",
        "    #     )\n",
        "    # else:\n",
        "    #     print(\"[INFO] XGBoost not available or no feature_importances_.\")\n",
        "\n",
        "print(\"\\n✅ Finished 02_train_classical_ml.ipynb\")\n",
        "print(\"   - Models: saved in results/models/\")\n",
        "print(\"   - Metrics: results/metrics/classical_ml_summary.csv\")\n",
        "print(\"   - Figures: results/figures/ (CM, LC)\") #FI 뺐음\n",
        "print(\"   - Next: use 03_analysis_report.ipynb for narrative analysis.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f37351af",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv (3.13.5)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
