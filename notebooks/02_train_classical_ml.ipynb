{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "90d7bee1",
      "metadata": {},
      "source": [
        "# 02_train\n",
        "- Author: \n",
        "- Date: 2025-10-23\n",
        "- Goal: 모델링/학습/통계/시각화\n",
        "- Input: \n",
        "- Output: \n",
        "- Metrics: acc@val, loss@train\n",
        "- Repro: seed=42, device=auto, config=../configs/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "0be6fdd6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# This cell is only needed if xgboost is not installed in your environment.\n",
        "# In Colab, run this once and then restart the runtime if necessary.\n",
        "# option Cell !!! \n",
        "# xgboost 안깔려 있으면 주석 풀고 실행!\n",
        "#!pip install -q xgboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "c4c1e62c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[OK] Libraries imported.\n",
            "[INFO] BASE_DIR       : /Users/jaehun_jung/colored-mnist-classification\n",
            "[INFO] PROCESSED_PATH : /Users/jaehun_jung/colored-mnist-classification/data/processed/colored_mnist/colored_mnist.npz\n",
            "[OK] Loaded npz keys: ['X_train', 'X_val', 'X_test', 'X_train_raw', 'X_val_raw', 'X_test_raw', 'y_digit_train', 'y_digit_val', 'y_digit_test', 'y_fg_train', 'y_fg_val', 'y_fg_test', 'y_bg_train', 'y_bg_val', 'y_bg_test', 'y_source_train', 'y_source_val', 'y_source_test']\n",
            "[INFO] Feature shapes (scaled):\n",
            "  X_train: (56000, 2352)\n",
            "  X_val  : (7000, 2352)\n",
            "  X_test : (7000, 2352)\n",
            "[INFO] Raw features are loaded as well, reserved for future use (e.g., PCA).\n",
            "[TASK] Digit classification (0-9).\n",
            "[INFO] Label shapes:\n",
            "  y_train: (56000,)\n",
            "  y_val  : (7000,)\n",
            "  y_test : (7000,)\n",
            "  num_classes: 10\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# 02_train_classical_ml.ipynb\n",
        "#\n",
        "# Train classical ML models (KNN, SVM, Decision Tree,\n",
        "# Random Forest, XGBoost) on Colored MNIST.\n",
        "#\n",
        "# - Uses preprocessed features from 01_preprocessing_colored_mnist.ipynb\n",
        "# - Supports 3 tasks:\n",
        "#     1) Digit classification (0-9)\n",
        "#     2) Foreground color classification (7 classes, ROYGBIV)\n",
        "#     3) Background color classification (7 classes, ROYGBIV)\n",
        "# ============================================================\n",
        "\n",
        "import os  # path handling\n",
        "import numpy as np  # numerical operations\n",
        "import matplotlib.pyplot as plt  # visualization\n",
        "import seaborn as sns  # nicer plots\n",
        "\n",
        "from sklearn.metrics import (  # evaluation metrics\n",
        "    accuracy_score,\n",
        "    precision_recall_fscore_support,\n",
        "    classification_report,\n",
        "    confusion_matrix,\n",
        ")\n",
        "from sklearn.model_selection import GridSearchCV  # hyperparameter tuning\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier  # KNN model\n",
        "from sklearn.svm import SVC  # SVM model\n",
        "from sklearn.tree import DecisionTreeClassifier  # Decision Tree model\n",
        "from sklearn.ensemble import RandomForestClassifier  # Random Forest model\n",
        "\n",
        "import xgboost as xgb  # XGBoost model\n",
        "\n",
        "# Matplotlib style (English only)\n",
        "plt.rcParams[\"font.family\"] = \"DejaVu Sans\"  # default font\n",
        "plt.rcParams[\"axes.unicode_minus\"] = False  # minus sign\n",
        "sns.set(style=\"whitegrid\")  # seaborn style\n",
        "\n",
        "RANDOM_STATE = 42  # global random seed for reproducibility\n",
        "\n",
        "print(\"[OK] Libraries imported.\")\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Resolve BASE_DIR and processed npz path\n",
        "# ------------------------------------------------------------\n",
        "cwd = os.getcwd()  # current working directory\n",
        "\n",
        "# If current folder is 'notebooks', use its parent as repo root\n",
        "if os.path.basename(cwd) == \"notebooks\":  # check folder name\n",
        "    BASE_DIR = os.path.dirname(cwd)  # go one level up\n",
        "else:\n",
        "    BASE_DIR = cwd  # otherwise use current directory as root\n",
        "\n",
        "PROCESSED_PATH = os.path.join(\n",
        "    BASE_DIR, \"data\", \"processed\", \"colored_mnist\", \"colored_mnist.npz\"\n",
        ")  # path to preprocessed npz\n",
        "\n",
        "print(f\"[INFO] BASE_DIR       : {BASE_DIR}\")\n",
        "print(f\"[INFO] PROCESSED_PATH : {PROCESSED_PATH}\")\n",
        "\n",
        "if not os.path.exists(PROCESSED_PATH):  # check if file exists\n",
        "    raise FileNotFoundError(\n",
        "        f\"[ERROR] Processed file not found at {PROCESSED_PATH}.\\n\"\n",
        "        f\"Please run 01_preprocessing_colored_mnist.ipynb first.\"\n",
        "    )\n",
        "\n",
        "data = np.load(PROCESSED_PATH)  # load npz file into memory\n",
        "print(\"[OK] Loaded npz keys:\", list(data.keys()))\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Extract features (scaled + raw)\n",
        "#   - X_*     : standardized features (mean=0, std=1) for classical ML\n",
        "#   - X_*_raw : flattened RGB in [0,1] (for PCA or other future use)\n",
        "# ------------------------------------------------------------\n",
        "X_train = data[\"X_train\"].astype(np.float32)  # scaled train features\n",
        "X_val   = data[\"X_val\"].astype(np.float32)    # scaled val features\n",
        "X_test  = data[\"X_test\"].astype(np.float32)   # scaled test features\n",
        "\n",
        "X_train_raw = data[\"X_train_raw\"].astype(np.float32)  # raw train features (0-1)\n",
        "X_val_raw   = data[\"X_val_raw\"].astype(np.float32)    # raw val features\n",
        "X_test_raw  = data[\"X_test_raw\"].astype(np.float32)   # raw test features\n",
        "\n",
        "print(\"[INFO] Feature shapes (scaled):\")\n",
        "print(\"  X_train:\", X_train.shape)\n",
        "print(\"  X_val  :\", X_val.shape)\n",
        "print(\"  X_test :\", X_test.shape)\n",
        "\n",
        "print(\"[INFO] Raw features are loaded as well, reserved for future use (e.g., PCA).\")\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Define color names (same order as in 01_preprocessing)\n",
        "# ------------------------------------------------------------\n",
        "COLOR_NAMES = [\"red\", \"orange\", \"yellow\", \"green\", \"blue\", \"indigo\", \"violet\"]  # 7 colors\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Select task: \"digit\", \"fg\", or \"bg\"\n",
        "# ------------------------------------------------------------\n",
        "TASK = \"digit\"  # change to \"fg\" or \"bg\" depending on experiment\n",
        "\n",
        "if TASK == \"digit\":\n",
        "    y_train = data[\"y_digit_train\"]  # digit labels for train\n",
        "    y_val   = data[\"y_digit_val\"]    # digit labels for val\n",
        "    y_test  = data[\"y_digit_test\"]   # digit labels for test\n",
        "    class_names = [str(i) for i in range(10)]  # class names \"0\"~\"9\"\n",
        "    print(\"[TASK] Digit classification (0-9).\")\n",
        "elif TASK == \"fg\":\n",
        "    y_train = data[\"y_fg_train\"]  # foreground color labels for train\n",
        "    y_val   = data[\"y_fg_val\"]    # foreground color labels for val\n",
        "    y_test  = data[\"y_fg_test\"]   # foreground color labels for test\n",
        "    class_names = COLOR_NAMES     # class names are color names\n",
        "    print(\"[TASK] Foreground color classification.\")\n",
        "elif TASK == \"bg\":\n",
        "    y_train = data[\"y_bg_train\"]  # background color labels for train\n",
        "    y_val   = data[\"y_bg_val\"]    # background color labels for val\n",
        "    y_test  = data[\"y_bg_test\"]   # background color labels for test\n",
        "    class_names = COLOR_NAMES     # class names are color names\n",
        "    print(\"[TASK] Background color classification.\")\n",
        "else:\n",
        "    raise ValueError(\"TASK must be one of: 'digit', 'fg', 'bg'.\")\n",
        "\n",
        "print(\"[INFO] Label shapes:\")\n",
        "print(\"  y_train:\", y_train.shape)\n",
        "print(\"  y_val  :\", y_val.shape)\n",
        "print(\"  y_test :\", y_test.shape)\n",
        "print(\"  num_classes:\", len(class_names))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "7e0a9292",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ------------------------------------------------------------\n",
        "# [Cell 2] Utility function to evaluate a classifier\n",
        "#   - Trains the model on (X_train, y_train)\n",
        "#   - Evaluates on both val and test sets\n",
        "#   - Prints accuracy / precision / recall / F1\n",
        "#   - Shows confusion matrix for each split\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "def evaluate_classifier(model,  # sklearn/xgboost-like estimator\n",
        "                        model_name: str,\n",
        "                        X_train, y_train,\n",
        "                        X_val, y_val,\n",
        "                        X_test, y_test,\n",
        "                        class_names):\n",
        "    \"\"\"Train and evaluate a classifier on val and test sets.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    model : estimator\n",
        "        Classifier implementing fit/predict.\n",
        "    model_name : str\n",
        "        Human-readable model name for printing.\n",
        "    X_train, y_train : array-like\n",
        "        Training features and labels.\n",
        "    X_val, y_val : array-like\n",
        "        Validation features and labels.\n",
        "    X_test, y_test : array-like\n",
        "        Test features and labels.\n",
        "    class_names : list of str\n",
        "        Names of classes, used for reports and confusion matrices.\n",
        "    \"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"[MODEL] {model_name}\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # --------- Train ---------\n",
        "    model.fit(X_train, y_train)  # fit classifier on training data\n",
        "    print(\"[OK] Training finished.\")\n",
        "\n",
        "    # --------- Validation evaluation ---------\n",
        "    y_val_pred = model.predict(X_val)  # predictions for validation set\n",
        "    acc_val = accuracy_score(y_val, y_val_pred)  # validation accuracy\n",
        "    pr_val, rc_val, f1_val, _ = precision_recall_fscore_support(\n",
        "        y_val, y_val_pred, average=\"weighted\", zero_division=0\n",
        "    )  # weighted precision/recall/F1\n",
        "\n",
        "    print(\"\\n[Val] Metrics:\")\n",
        "    print(f\"  Accuracy : {acc_val:.4f}\")\n",
        "    print(f\"  Precision: {pr_val:.4f}\")\n",
        "    print(f\"  Recall   : {rc_val:.4f}\")\n",
        "    print(f\"  F1-score : {f1_val:.4f}\")\n",
        "    print(\"\\n[Val] Classification report:\")\n",
        "    print(classification_report(y_val, y_val_pred, target_names=class_names, zero_division=0))\n",
        "\n",
        "    cm_val = confusion_matrix(y_val, y_val_pred)  # confusion matrix for validation\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    sns.heatmap(cm_val, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "                xticklabels=class_names, yticklabels=class_names)\n",
        "    plt.title(f\"{model_name} - Val Confusion Matrix\")\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"True\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # --------- Test evaluation ---------\n",
        "    y_test_pred = model.predict(X_test)  # predictions for test set\n",
        "    acc_test = accuracy_score(y_test, y_test_pred)  # test accuracy\n",
        "    pr_test, rc_test, f1_test, _ = precision_recall_fscore_support(\n",
        "        y_test, y_test_pred, average=\"weighted\", zero_division=0\n",
        "    )\n",
        "\n",
        "    print(\"\\n[Test] Metrics:\")\n",
        "    print(f\"  Accuracy : {acc_test:.4f}\")\n",
        "    print(f\"  Precision: {pr_test:.4f}\")\n",
        "    print(f\"  Recall   : {rc_test:.4f}\")\n",
        "    print(f\"  F1-score : {f1_test:.4f}\")\n",
        "    print(\"\\n[Test] Classification report:\")\n",
        "    print(classification_report(y_test, y_test_pred, target_names=class_names, zero_division=0))\n",
        "\n",
        "    cm_test = confusion_matrix(y_test, y_test_pred)  # confusion matrix for test\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    sns.heatmap(cm_test, annot=True, fmt=\"d\", cmap=\"Greens\",\n",
        "                xticklabels=class_names, yticklabels=class_names)\n",
        "    plt.title(f\"{model_name} - Test Confusion Matrix\")\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"True\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Return dictionary for later summary if needed\n",
        "    return {\n",
        "        \"model\": model,\n",
        "        \"acc_val\": acc_val,\n",
        "        \"f1_val\": f1_val,\n",
        "        \"acc_test\": acc_test,\n",
        "        \"f1_test\": f1_test,\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "36cbf0f9",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[KNN] Starting hyperparameter search...\n",
            "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n",
            "[CV] END ................n_neighbors=3, p=2, weights=uniform; total time= 2.1min\n",
            "[CV] END ...............n_neighbors=3, p=2, weights=distance; total time= 2.1min\n",
            "[CV] END ................n_neighbors=3, p=2, weights=uniform; total time= 2.1min\n",
            "[CV] END ................n_neighbors=3, p=2, weights=uniform; total time= 2.1min\n",
            "[CV] END ...............n_neighbors=3, p=2, weights=distance; total time= 2.0min\n",
            "[CV] END ...............n_neighbors=3, p=2, weights=distance; total time= 2.0min\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[5], line 28\u001b[0m\n\u001b[1;32m     18\u001b[0m knn_grid \u001b[38;5;241m=\u001b[39m GridSearchCV(\n\u001b[1;32m     19\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mknn_base,     \u001b[38;5;66;03m# base KNN model\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     param_grid\u001b[38;5;241m=\u001b[39mparam_grid_knn,  \u001b[38;5;66;03m# parameter search space\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,              \u001b[38;5;66;03m# print progress\u001b[39;00m\n\u001b[1;32m     25\u001b[0m )\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[KNN] Starting hyperparameter search...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 28\u001b[0m knn_grid\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)  \u001b[38;5;66;03m# fit GridSearchCV on training data\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m[KNN] Best parameters:\u001b[39m\u001b[38;5;124m\"\u001b[39m, knn_grid\u001b[38;5;241m.\u001b[39mbest_params_)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[KNN] Best CV accuracy:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mknn_grid\u001b[38;5;241m.\u001b[39mbest_score_\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py:1365\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1358\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1360\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1361\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1362\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1363\u001b[0m     )\n\u001b[1;32m   1364\u001b[0m ):\n\u001b[0;32m-> 1365\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1051\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m   1045\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m   1046\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m   1047\u001b[0m     )\n\u001b[1;32m   1049\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m-> 1051\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_search(evaluate_candidates)\n\u001b[1;32m   1053\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m   1054\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m   1055\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1605\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1603\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1604\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1605\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_grid))\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_search.py:997\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    989\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    990\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    991\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    992\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    993\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[1;32m    994\u001b[0m         )\n\u001b[1;32m    995\u001b[0m     )\n\u001b[0;32m--> 997\u001b[0m out \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[1;32m    998\u001b[0m     delayed(_fit_and_score)(\n\u001b[1;32m    999\u001b[0m         clone(base_estimator),\n\u001b[1;32m   1000\u001b[0m         X,\n\u001b[1;32m   1001\u001b[0m         y,\n\u001b[1;32m   1002\u001b[0m         train\u001b[38;5;241m=\u001b[39mtrain,\n\u001b[1;32m   1003\u001b[0m         test\u001b[38;5;241m=\u001b[39mtest,\n\u001b[1;32m   1004\u001b[0m         parameters\u001b[38;5;241m=\u001b[39mparameters,\n\u001b[1;32m   1005\u001b[0m         split_progress\u001b[38;5;241m=\u001b[39m(split_idx, n_splits),\n\u001b[1;32m   1006\u001b[0m         candidate_progress\u001b[38;5;241m=\u001b[39m(cand_idx, n_candidates),\n\u001b[1;32m   1007\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_and_score_kwargs,\n\u001b[1;32m   1008\u001b[0m     )\n\u001b[1;32m   1009\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[38;5;129;01min\u001b[39;00m product(\n\u001b[1;32m   1010\u001b[0m         \u001b[38;5;28menumerate\u001b[39m(candidate_params),\n\u001b[1;32m   1011\u001b[0m         \u001b[38;5;28menumerate\u001b[39m(cv\u001b[38;5;241m.\u001b[39msplit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrouted_params\u001b[38;5;241m.\u001b[39msplitter\u001b[38;5;241m.\u001b[39msplit)),\n\u001b[1;32m   1012\u001b[0m     )\n\u001b[1;32m   1013\u001b[0m )\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1016\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1017\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1018\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1019\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1020\u001b[0m     )\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/parallel.py:82\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     73\u001b[0m warning_filters \u001b[38;5;241m=\u001b[39m warnings\u001b[38;5;241m.\u001b[39mfilters\n\u001b[1;32m     74\u001b[0m iterable_with_config_and_warning_filters \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     75\u001b[0m     (\n\u001b[1;32m     76\u001b[0m         _with_config_and_warning_filters(delayed_func, config, warning_filters),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     81\u001b[0m )\n\u001b[0;32m---> 82\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config_and_warning_filters)\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/joblib/parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[1;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/joblib/parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/joblib/parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[1;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[0;32m-> 1762\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[1;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[1;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[1;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# ------------------------------------------------------------\n",
        "# [Cell 3] KNN with hyperparameter tuning (GridSearchCV)\n",
        "#   - Uses scaled features X_train / X_val / X_test\n",
        "#   - Parameter search over k, weights, and distance metric\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "# Define base KNN classifier\n",
        "knn_base = KNeighborsClassifier()  # default KNN classifier\n",
        "\n",
        "# Define hyperparameter search space for KNN\n",
        "param_grid_knn = {\n",
        "    \"n_neighbors\": [3, 5, 7, 9],         # number of neighbors\n",
        "    \"weights\": [\"uniform\", \"distance\"],  # uniform weights vs distance weights\n",
        "    \"p\": [1, 2],                         # 1: Manhattan, 2: Euclidean\n",
        "}\n",
        "\n",
        "# GridSearchCV configuration\n",
        "knn_grid = GridSearchCV(\n",
        "    estimator=knn_base,     # base KNN model\n",
        "    param_grid=param_grid_knn,  # parameter search space\n",
        "    scoring=\"accuracy\",     # optimization metric (can change to 'f1_weighted')\n",
        "    cv=3,                   # 3-fold cross-validation on training set\n",
        "    n_jobs=-1,              # use all available CPU cores\n",
        "    verbose=2,              # print progress\n",
        ")\n",
        "\n",
        "print(\"[KNN] Starting hyperparameter search...\")\n",
        "knn_grid.fit(X_train, y_train)  # fit GridSearchCV on training data\n",
        "\n",
        "print(\"\\n[KNN] Best parameters:\", knn_grid.best_params_)\n",
        "print(\"[KNN] Best CV accuracy:\", f\"{knn_grid.best_score_:.4f}\")\n",
        "\n",
        "# Extract best model from grid search\n",
        "knn_best = knn_grid.best_estimator_  # best KNN model according to CV\n",
        "\n",
        "# Evaluate best KNN model using common evaluation function\n",
        "results_knn = evaluate_classifier(\n",
        "    model=knn_best,\n",
        "    model_name=f\"KNN (best grid: {knn_grid.best_params_})\",\n",
        "    X_train=X_train,\n",
        "    y_train=y_train,\n",
        "    X_val=X_val,\n",
        "    y_val=y_val,\n",
        "    X_test=X_test,\n",
        "    y_test=y_test,\n",
        "    class_names=class_names,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "596997bf",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ------------------------------------------------------------\n",
        "# [Cell 4] SVM with hyperparameter tuning (GridSearchCV)\n",
        "#   - Uses RBF kernel (non-linear decision boundary)\n",
        "#   - Uses scaled features X_train / X_val / X_test\n",
        "#   - Parameter search over C and gamma\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "# Base SVM classifier (RBF kernel)\n",
        "svm_base = SVC(\n",
        "    kernel=\"rbf\",        # radial basis function kernel\n",
        "    probability=False,   # set True if probability estimates are needed\n",
        "    random_state=RANDOM_STATE,\n",
        ")\n",
        "\n",
        "# Hyperparameter search space for SVM\n",
        "# Note: Keep grid small to avoid very long training time\n",
        "param_grid_svm = {\n",
        "    \"C\": [1.0, 5.0, 10.0],        # regularization strength\n",
        "    \"gamma\": [\"scale\", 0.01, 0.001],  # kernel coefficient\n",
        "}\n",
        "\n",
        "svm_grid = GridSearchCV(\n",
        "    estimator=svm_base,       # base SVM model\n",
        "    param_grid=param_grid_svm,  # search space for C and gamma\n",
        "    scoring=\"accuracy\",       # optimization metric\n",
        "    cv=3,                     # 3-fold cross-validation\n",
        "    n_jobs=-1,                # parallel jobs\n",
        "    verbose=2,                # print progress\n",
        ")\n",
        "\n",
        "print(\"[SVM] Starting hyperparameter search...\")\n",
        "svm_grid.fit(X_train, y_train)  # fit GridSearchCV\n",
        "\n",
        "print(\"\\n[SVM] Best parameters:\", svm_grid.best_params_)\n",
        "print(\"[SVM] Best CV accuracy:\", f\"{svm_grid.best_score_:.4f}\")\n",
        "\n",
        "svm_best = svm_grid.best_estimator_  # best SVM model\n",
        "\n",
        "# Evaluate best SVM model\n",
        "results_svm = evaluate_classifier(\n",
        "    model=svm_best,\n",
        "    model_name=f\"SVM (best grid: {svm_grid.best_params_})\",\n",
        "    X_train=X_train,\n",
        "    y_train=y_train,\n",
        "    X_val=X_val,\n",
        "    y_val=y_val,\n",
        "    X_test=X_test,\n",
        "    y_test=y_test,\n",
        "    class_names=class_names,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7c3e5a8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ------------------------------------------------------------\n",
        "# [Cell 5] Decision Tree (single tree baseline)\n",
        "#   - Uses scaled features (X_train), but tree-based models do not strictly require scaling\n",
        "#   - Good for interpretability and feature importance (later)\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "dt_clf = DecisionTreeClassifier(\n",
        "    criterion=\"entropy\",       # splitting criterion ('gini' or 'entropy')\n",
        "    max_depth=None,         # allow the tree to grow until pure or min_samples constraints\n",
        "    min_samples_split=2,    # minimum samples required to split an internal node\n",
        "    min_samples_leaf=1,     # minimum samples required at a leaf node\n",
        "    random_state=RANDOM_STATE,\n",
        ")\n",
        "\n",
        "results_dt = evaluate_classifier(\n",
        "    model=dt_clf,\n",
        "    model_name=\"Decision Tree\",\n",
        "    X_train=X_train,\n",
        "    y_train=y_train,\n",
        "    X_val=X_val,\n",
        "    y_val=y_val,\n",
        "    X_test=X_test,\n",
        "    y_test=y_test,\n",
        "    class_names=class_names,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "320f1289",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ------------------------------------------------------------\n",
        "# [Cell 6] Random Forest\n",
        "#   - Ensemble of decision trees\n",
        "#   - Usually more stable and accurate than a single tree\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "rf_clf = RandomForestClassifier(\n",
        "    n_estimators=200,     # number of trees in the forest (can increase if time allows)\n",
        "    criterion=\"entropy\",     # splitting criterion\n",
        "    max_depth=None,       # allow deep trees; can limit for speed\n",
        "    min_samples_split=2,  # minimum samples to split\n",
        "    min_samples_leaf=1,   # minimum samples at leaf\n",
        "    n_jobs=-1,            # use all CPU cores\n",
        "    random_state=RANDOM_STATE,\n",
        ")\n",
        "\n",
        "results_rf = evaluate_classifier(\n",
        "    model=rf_clf,\n",
        "    model_name=\"Random Forest (200 trees)\",\n",
        "    X_train=X_train,\n",
        "    y_train=y_train,\n",
        "    X_val=X_val,\n",
        "    y_val=y_val,\n",
        "    X_test=X_test,\n",
        "    y_test=y_test,\n",
        "    class_names=class_names,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93e8ce16",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ------------------------------------------------------------\n",
        "# [Cell 7] XGBoost (gradient boosting trees)\n",
        "#   - Uses xgboost.XGBClassifier\n",
        "#   - Supports early stopping with validation set\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "# Number of classes for the current task\n",
        "num_classes = len(class_names)  # number of output classes\n",
        "\n",
        "xgb_clf = xgb.XGBClassifier(\n",
        "    objective=\"multi:softprob\",  # multi-class probability output\n",
        "    num_class=num_classes,       # number of classes\n",
        "    n_estimators=300,            # maximum number of boosting rounds\n",
        "    learning_rate=0.1,           # step size shrinkage\n",
        "    max_depth=6,                 # depth of individual trees\n",
        "    subsample=0.8,               # row sampling\n",
        "    colsample_bytree=0.8,        # column sampling\n",
        "    tree_method=\"hist\",          # fast histogram-based method\n",
        "    eval_metric=\"mlogloss\",      # evaluation metric\n",
        "    random_state=RANDOM_STATE,\n",
        "    n_jobs=-1,                   # parallel threads\n",
        ")\n",
        "\n",
        "print(\"[XGB] Starting training with early stopping...\")\n",
        "\n",
        "# XGBoost can use early_stopping_rounds with eval_set\n",
        "xgb_clf.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    eval_set=[(X_val, y_val)],  # use validation set for early stopping\n",
        "    verbose=True,               # print evaluation metric for each round\n",
        "    early_stopping_rounds=20,   # stop if no improvement for 20 rounds\n",
        ")\n",
        "\n",
        "print(\"[XGB] Best iteration:\", xgb_clf.best_iteration)\n",
        "\n",
        "# Evaluate using common function\n",
        "results_xgb = evaluate_classifier(\n",
        "    model=xgb_clf,\n",
        "    model_name=\"XGBoost (early stopping)\",\n",
        "    X_train=X_train,   # note: model is already fitted; this call will refit once more\n",
        "    y_train=y_train,\n",
        "    X_val=X_val,\n",
        "    y_val=y_val,\n",
        "    X_test=X_test,\n",
        "    y_test=y_test,\n",
        "    class_names=class_names,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7306a9fa",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ------------------------------------------------------------\n",
        "# [Cell 8] Optional: summarize validation & test metrics\n",
        "#   - Only works for models that have been executed above\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "summary = []\n",
        "\n",
        "if \"results_knn\" in globals():\n",
        "    summary.append([\"KNN\", results_knn[\"acc_val\"], results_knn[\"f1_val\"],\n",
        "                    results_knn[\"acc_test\"], results_knn[\"f1_test\"]])\n",
        "\n",
        "if \"results_svm\" in globals():\n",
        "    summary.append([\"SVM\", results_svm[\"acc_val\"], results_svm[\"f1_val\"],\n",
        "                    results_svm[\"acc_test\"], results_svm[\"f1_test\"]])\n",
        "\n",
        "if \"results_dt\" in globals():\n",
        "    summary.append([\"Decision Tree\", results_dt[\"acc_val\"], results_dt[\"f1_val\"],\n",
        "                    results_dt[\"acc_test\"], results_dt[\"f1_test\"]])\n",
        "\n",
        "if \"results_rf\" in globals():\n",
        "    summary.append([\"Random Forest\", results_rf[\"acc_val\"], results_rf[\"f1_val\"],\n",
        "                    results_rf[\"acc_test\"], results_rf[\"f1_test\"]])\n",
        "\n",
        "if \"results_xgb\" in globals():\n",
        "    summary.append([\"XGBoost\", results_xgb[\"acc_val\"], results_xgb[\"f1_val\"],\n",
        "                    results_xgb[\"acc_test\"], results_xgb[\"f1_test\"]])\n",
        "\n",
        "if len(summary) > 0:\n",
        "    header = [\"Model\", \"Val Acc\", \"Val F1\", \"Test Acc\", \"Test F1\"]\n",
        "    print(\"\\n=== Summary (Val / Test) ===\")\n",
        "    row_fmt = \"{:<15} {:>7.4f} {:>7.4f} {:>9.4f} {:>9.4f}\"\n",
        "    print(\"{:<15} {:>7} {:>7} {:>9} {:>9}\".format(*header))\n",
        "    for row in summary:\n",
        "        print(row_fmt.format(row[0], row[1], row[2], row[3], row[4]))\n",
        "else:\n",
        "    print(\"[INFO] No models have been run yet. Execute KNN/SVM/DT/RF/XGB cells first.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1250085",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
